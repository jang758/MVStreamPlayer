"""
StreamPlayer - ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ ë°±ì—”ë“œ ì„œë²„
ì¿ í‚¤ ê¸°ë°˜ ì¸ì¦ì„ ì§€ì›í•˜ë©° yt-dlpë¥¼ í†µí•´ ì˜ìƒ ìŠ¤íŠ¸ë¦¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
MissAV ë“± P.A.C.K.E.R. ë‚œë…í™” ì‚¬ì´íŠ¸ëŠ” ì»¤ìŠ¤í…€ ì¶”ì¶œê¸°ë¡œ í´ë°±í•©ë‹ˆë‹¤.
"""

import os
import json
import time
import hashlib
import re
import threading
import urllib.parse
from pathlib import Path
from flask import Flask, request, jsonify, render_template, Response, send_file, stream_with_context
import yt_dlp
import requests
from bs4 import BeautifulSoup

app = Flask(__name__)
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0  # ì •ì  íŒŒì¼ ìºì‹œ ë¹„í™œì„±í™”

@app.after_request
def _add_no_cache_headers(response):
    """ì •ì  íŒŒì¼ì— ìºì‹œ ë°©ì§€ í—¤ë” ì¶”ê°€ (WebView2 ìºì‹œ ë¬´íš¨í™”)"""
    if 'static' in request.path or request.path == '/':
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
    return response

BASE_DIR = Path(__file__).parent
COOKIES_FILE = BASE_DIR / "cookies.txt"
DOWNLOADS_DIR = BASE_DIR / "downloads"
DATA_FILE = BASE_DIR / "data.json"

DOWNLOADS_DIR.mkdir(exist_ok=True)

USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'

# MissAV ë“± ëŒ€ìƒ ë„ë©”ì¸
CUSTOM_DOMAINS = ['missav.ws', 'missav.ai', 'missav.com', 'njavtv.com']

# pywebview ì°½ ì°¸ì¡° (í•­ìƒ ìœ„ ê¸°ëŠ¥ ë“±)
_webview_window = None
_search_window = None
_webview_ready = threading.Event()   # guilib ì´ˆê¸°í™” ì™„ë£Œ ì‹œê·¸ë„

def set_webview_window(window):
    global _webview_window
    _webview_window = window

def set_search_window(window):
    global _search_window
    _search_window = window


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ì´íŠ¸ ë¸Œë¼ìš°ì € - pywebview JS API ë¸Œë¦¿ì§€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class BrowseAPI:
    """ê²€ìƒ‰/íƒìƒ‰ ì°½ì—ì„œ pywebview JS ë¸Œë¦¿ì§€ë¥¼ í†µí•´ í˜¸ì¶œë˜ëŠ” API.
    MissAV ì‚¬ì´íŠ¸ë¥¼ ì§ì ‘ íƒìƒ‰í•˜ë©´ì„œ ëŒ€ê¸°ì—´ì— ì¶”ê°€í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤."""

    def add_to_queue(self, url):
        """ì˜ìƒ URLì„ ëŒ€ê¸°ì—´ì— ì¶”ê°€í•©ë‹ˆë‹¤. (JSì—ì„œ í˜¸ì¶œ)
        ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ìµœëŒ€ 2íšŒ ì¬ì‹œë„í•©ë‹ˆë‹¤."""
        if not url or not url.strip():
            return {"error": "URLì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
        url = url.strip()
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±° (missav URL ì •ê·œí™”)
        if '?' in url:
            url = url.split('?')[0]

        uid = _url_id(url)
        # ì¤‘ë³µ ì²´í¬ ë¨¼ì €
        data = _load_data()
        if any(item["id"] == uid for item in data["queue"]):
            return {"error": "ì´ë¯¸ ëŒ€ê¸°ì—´ì— ìˆìŠµë‹ˆë‹¤.", "duplicate": True, "title": url}

        # ì¶”ì¶œ ì‹œë„ (ìµœëŒ€ 2íšŒ ì¬ì‹œë„)
        last_error = None
        info = None
        for attempt in range(3):
            try:
                info = _extract_info(url)
                break
            except Exception as e:
                last_error = str(e)
                print(f"  [íƒìƒ‰ì°½] ì¶”ì¶œ ì‹œë„ {attempt+1}/3 ì‹¤íŒ¨: {last_error}")
                if attempt < 2:
                    time.sleep(1)

        if info is None:
            return {"error": f"ì˜ìƒ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ (3íšŒ ì‹œë„): {last_error}"}

        entry = {
            "id": uid,
            "url": url,
            "title": info.get("title", url),
            "duration": info.get("duration", 0),
            "thumbnail": info.get("thumbnail", ""),
            "added_at": time.time(),
            "stream_url": info.get("url", ""),
            "http_headers": info.get("http_headers", {}),
            "variants": info.get("_variants", []),
        }

        # ë‹¤ì‹œ í•œë²ˆ ì¤‘ë³µ ì²´í¬ (ì¶”ì¶œ ì¤‘ ë‹¤ë¥¸ ê³³ì—ì„œ ì¶”ê°€ë˜ì—ˆì„ ìˆ˜ ìˆìŒ)
        data = _load_data()
        if any(item["id"] == uid for item in data["queue"]):
            return {"error": "ì´ë¯¸ ëŒ€ê¸°ì—´ì— ìˆìŠµë‹ˆë‹¤.", "duplicate": True, "title": entry["title"]}
        data["queue"].append(entry)
        _save_data(data)

        # ì €ì¥ ê²€ì¦
        verify = _load_data()
        saved = any(item["id"] == uid for item in verify["queue"])
        if not saved:
            return {"error": "ì €ì¥ ì‹¤íŒ¨ â€” ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "save_failed": True}

        print(f"  [íƒìƒ‰ì°½] ëŒ€ê¸°ì—´ ì¶”ê°€: {entry['title'][:60]}")
        return {"ok": True, "title": entry["title"], "id": uid}

    def get_queue_urls(self):
        """ëŒ€ê¸°ì—´ì— ìˆëŠ” URL ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤. (JSì—ì„œ í˜¸ì¶œ)"""
        data = _load_data()
        return [item["url"] for item in data["queue"]]

    def get_queue_count(self):
        """ëŒ€ê¸°ì—´ í•­ëª© ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (JSì—ì„œ í˜¸ì¶œ)"""
        data = _load_data()
        return len(data["queue"])

    def open_new_tab(self, url):
        """ìƒˆ íƒìƒ‰ ì°½(íƒ­)ì„ ì—´ì–´ ì§€ì • URLë¡œ ì´ë™í•©ë‹ˆë‹¤. (JSì—ì„œ í˜¸ì¶œ)"""
        if not url:
            return {"error": "URLì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
        try:
            _open_browse_tab(url)
            return {"ok": True}
        except Exception as e:
            return {"error": str(e)}

_browse_api = BrowseAPI()


# â”€â”€ ì‚¬ì´íŠ¸ ë¸Œë¼ìš°ì €ìš© ì¸ì ì…˜ JavaScript â”€â”€
_BROWSE_INJECT_JS = r"""
(function() {
    'use strict';
    if (document.getElementById('sp-toolbar')) return;

    /* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
       ê´‘ê³  ì°¨ë‹¨ â€” window.open / íŒì—… / ë¦¬ë‹¤ì´ë ‰íŠ¸ ë°©ì§€
       â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */
    (function blockAds() {
        /* 1) window.open ì™„ì „ ì°¨ë‹¨ â€” fake window ê°ì²´ ë°˜í™˜ìœ¼ë¡œ ê´‘ê³  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì†ì„ */
        const _origOpen = window.open;
        function _makeFakeWindow(url) {
            const fw = {
                closed: false, opener: window, name: '',
                location: { href: url || '', replace: function(){}, assign: function(){} },
                document: { write: function(){}, writeln: function(){}, close: function(){}, open: function(){ return this; },
                             readyState: 'complete', createElement: function(){ return document.createElement('div'); },
                             body: document.createElement('div'), head: document.createElement('head') },
                navigator: window.navigator,
                close: function() { fw.closed = true; },
                focus: function() {}, blur: function() {},
                postMessage: function() {},
                addEventListener: function() {}, removeEventListener: function() {},
                dispatchEvent: function() { return true; },
                setTimeout: function(fn,ms){ return window.setTimeout(fn,ms); },
                setInterval: function(fn,ms){ return window.setInterval(fn,ms); },
                clearTimeout: function(id){ window.clearTimeout(id); },
                clearInterval: function(id){ window.clearInterval(id); },
                Math: window.Math, Date: window.Date, JSON: window.JSON,
                atob: window.atob, btoa: window.btoa,
                innerWidth: 1024, innerHeight: 768,
                screen: window.screen,
                XMLHttpRequest: window.XMLHttpRequest,
                fetch: window.fetch,
            };
            /* ìë™ìœ¼ë¡œ ì ì‹œ í›„ ë‹«íŒ ê²ƒìœ¼ë¡œ í‘œì‹œ */
            setTimeout(() => { fw.closed = true; }, 2000);
            return fw;
        }
        window.open = function(url, target, features) {
            console.log('[SP AdBlock] window.open ì°¨ë‹¨ (fake window ë°˜í™˜):', url && url.substring(0, 80));
            return _makeFakeWindow(url);
        };

        /* 2) ê´‘ê³  ì˜¤ë²„ë ˆì´ / íŒì—… ë ˆì´ì–´ ì£¼ê¸°ì  ì œê±° */
        function removeAdElements() {
            /* ì „í˜•ì ì¸ ê´‘ê³  ì˜¤ë²„ë ˆì´ ì„ íƒì */
            const adSelectors = [
                'div[id*="pop"]', 'div[class*="pop"]',
                'div[id*="overlay"]', 'div[class*="overlay"]',
                'div[id*="banner"]', 'div[class*="banner"]',
                'div[id*="ad-"]', 'div[class*="ad-"]',
                'div[id*="ads"]', 'div[class*="ads"]',
                'iframe[src*="ad"]', 'iframe[src*="pop"]',
                'iframe[src*="banner"]',
                'a[href*="redirect"]', 'a[href*="click"]',
                '.exo', '.exo_wrapper',
                '[id^="div-gpt-ad"]',
                'div[style*="z-index: 2147483647"]',
            ];
            adSelectors.forEach(sel => {
                document.querySelectorAll(sel).forEach(el => {
                    /* SP íˆ´ë°” ìì²´ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ */
                    if (el.id === 'sp-toolbar' || el.closest('#sp-toolbar')) return;
                    if (el.id === 'sp-ctx-menu' || el.closest('#sp-ctx-menu')) return;
                    el.remove();
                });
            });
        }

        /* 3) í´ë¦­ í•˜ì´ì¬í‚¹ ì°¨ë‹¨ â€” ë™ì˜ìƒ ì˜ì—­ ë°–ì˜ íˆ¬ëª… ì˜¤ë²„ë ˆì´ í´ë¦­ ë°©ì§€ */
        document.addEventListener('click', function(e) {
            const el = e.target;
            /* íˆ¬ëª… ì „ì²´í™”ë©´ div (ê´‘ê³  íŠ¸ë¦¬ê±°) ê°ì§€ */
            if (el.tagName === 'DIV' || el.tagName === 'A') {
                const cs = getComputedStyle(el);
                const rect = el.getBoundingClientRect();
                /* í™”ë©´ ëŒ€ë¶€ë¶„ì„ ë®ëŠ” íˆ¬ëª…/ë°˜íˆ¬ëª… div â†’ ê´‘ê³  */
                if (rect.width > window.innerWidth * 0.5 && rect.height > window.innerHeight * 0.5) {
                    if (parseFloat(cs.opacity) < 0.15 || cs.visibility === 'hidden' ||
                        cs.pointerEvents === 'auto' && cs.background === 'transparent' ||
                        cs.zIndex > 99999) {
                        e.stopPropagation();
                        e.preventDefault();
                        el.remove();
                        console.log('[SP AdBlock] íˆ¬ëª… ì˜¤ë²„ë ˆì´ í´ë¦­ ì°¨ë‹¨+ì œê±°');
                        return false;
                    }
                }
            }
            /* ì™¸ë¶€ ë„ë©”ì¸ ë§í¬ í´ë¦­ ì°¨ë‹¨ */
            const link = el.closest('a[href]');
            if (link) {
                try {
                    const href = link.href;
                    const u = new URL(href);
                    const dominated = ['missav.ws','missav.ai','missav.com','njavtv.com'];
                    const isSameSite = dominated.some(d => u.hostname.endsWith(d));
                    if (!isSameSite && !href.startsWith('javascript:')) {
                        /* ì™¸ë¶€ ê´‘ê³  ë§í¬ */
                        e.stopPropagation();
                        e.preventDefault();
                        console.log('[SP AdBlock] ì™¸ë¶€ ë§í¬ ì°¨ë‹¨:', href.substring(0, 80));
                        return false;
                    }
                } catch {}
            }
        }, true);

        /* 4) beforeunload ì‹œ ê´‘ê³  ìŠ¤í¬ë¦½íŠ¸ê°€ ë¼ì–´ë“œëŠ” ê²ƒ ë°©ì§€ */
        Object.defineProperty(window, 'onbeforeunload', {
            get: () => null,
            set: () => {},
            configurable: false,
        });

        /* 5) ì£¼ê¸°ì  ê´‘ê³  ìš”ì†Œ ì •ë¦¬ (1ì´ˆ ê°„ê²©, 10íšŒ â†’ ì´í›„ 5ì´ˆ ê°„ê²©) */
        let adCleanCount = 0;
        const adCleanTimer = setInterval(() => {
            removeAdElements();
            adCleanCount++;
            if (adCleanCount >= 10) {
                clearInterval(adCleanTimer);
                /* ì´í›„ 5ì´ˆ ê°„ê²©ìœ¼ë¡œ ê³„ì† */
                setInterval(removeAdElements, 5000);
            }
        }, 1000);

        console.log('[SP AdBlock] ê´‘ê³  ì°¨ë‹¨ í™œì„±í™”');
    })();

    /* â”€â”€ ìƒíƒœ â”€â”€ */
    let addedUrls = new Set();
    let queueCount = 0;

    /* â”€â”€ ë¹„ë””ì˜¤ URL íŒë³„ (ì—¬ëŸ¬ íŒ¨í„´ ëŒ€ì‘) â”€â”€ */
    function isVideoUrl(url) {
        try {
            const p = new URL(url, location.origin).pathname.replace(/\/+$/, '');
            const parts = p.split('/').filter(Boolean);
            if (parts.length === 0) return false;
            const slug = parts[parts.length - 1];
            if (!/[a-zA-Z]/.test(slug)) return false;
            if (!/-\d/.test(slug)) return false;
            if (slug.length < 4) return false;
            const exclude = ['search','genres','actresses','makers','labels','tags','uncensored-leak','today-hot','weekly-hot','monthly-hot','new','release','login','register','dm','playlist'];
            if (exclude.includes(slug) || exclude.some(ex => parts.includes(ex) && parts.indexOf(ex) === parts.length - 1)) return false;
            return true;
        } catch { return false; }
    }
    function getFullUrl(href) {
        try { return new URL(href, location.origin).href.split('?')[0]; }
        catch { return null; }
    }

    /* â”€â”€ CSS â”€â”€ */
    const style = document.createElement('style');
    style.textContent = `
        #sp-toolbar {
            position: fixed; bottom: 0; left: 0; right: 0; height: 40px;
            background: rgba(15,15,20,0.95); backdrop-filter: blur(12px);
            display: flex; align-items: center; padding: 0 12px;
            z-index: 2147483647; font-family: -apple-system, 'Segoe UI', sans-serif;
            color: #e0e0e0; gap: 6px; border-top: 1px solid rgba(74,158,255,0.3);
            box-shadow: 0 -2px 12px rgba(0,0,0,0.5);
        }
        #sp-toolbar .sp-brand {
            font-weight: 700; color: #4a9eff; font-size: 12px;
            white-space: nowrap; user-select: none;
        }
        #sp-toolbar .sp-btn {
            background: #4a9eff; color: #fff; border: none;
            padding: 4px 12px; border-radius: 4px; font-weight: 600;
            cursor: pointer; font-size: 11px; white-space: nowrap;
            transition: all 0.15s;
        }
        #sp-toolbar .sp-btn:hover { background: #6bb3ff; }
        #sp-toolbar .sp-btn.sp-added { background: #4caf50; cursor: default; }
        #sp-toolbar .sp-btn.sp-dup { background: #ff9800; cursor: default; }
        #sp-toolbar .sp-btn:disabled { opacity: 0.6; cursor: wait; }
        #sp-toolbar .sp-btn.sp-na { background: #555; color: #999; cursor: default; }
        #sp-toolbar .sp-icon-btn {
            background: none; border: 1px solid rgba(255,255,255,0.2);
            color: #ccc; width: 28px; height: 28px; border-radius: 4px;
            cursor: pointer; font-size: 14px; display: flex;
            align-items: center; justify-content: center;
            transition: all 0.15s; padding: 0;
        }
        #sp-toolbar .sp-icon-btn:hover { border-color: #4a9eff; color: #4a9eff; background: rgba(74,158,255,0.1); }
        #sp-toolbar .sp-icon-btn:active { transform: scale(0.92); }
        #sp-toolbar .sp-status {
            font-size: 11px; color: #aaa; flex: 1;
            overflow: hidden; text-overflow: ellipsis; white-space: nowrap;
        }
        #sp-toolbar .sp-count {
            font-size: 11px; color: #888; white-space: nowrap;
        }
        /* ì¹´ë“œ ì˜¤ë²„ë ˆì´ ë²„íŠ¼ */
        .sp-card-btn {
            position: absolute; top: 6px; right: 6px;
            width: 28px; height: 28px; border-radius: 50%;
            background: rgba(74,158,255,0.9); color: #fff;
            border: none; font-size: 16px; font-weight: 700;
            cursor: pointer; display: flex; align-items: center;
            justify-content: center; z-index: 100;
            transition: all 0.15s; line-height: 1;
            box-shadow: 0 2px 6px rgba(0,0,0,0.4);
        }
        .sp-card-btn:hover { background: #6bb3ff; transform: scale(1.1); }
        .sp-card-btn.sp-card-added { background: rgba(76,175,80,0.9); cursor: default; font-size: 12px; }
        .sp-card-btn:disabled { opacity: 0.6; cursor: wait; }
        /* ì»¤ìŠ¤í…€ ì»¨í…ìŠ¤íŠ¸ ë©”ë‰´ */
        #sp-ctx-menu {
            position: fixed; z-index: 2147483647;
            background: rgba(30,30,38,0.98); backdrop-filter: blur(12px);
            border: 1px solid rgba(255,255,255,0.15); border-radius: 6px;
            box-shadow: 0 6px 24px rgba(0,0,0,0.6); padding: 4px;
            min-width: 180px; font-size: 13px; display: none;
        }
        #sp-ctx-menu .sp-ctx-item {
            padding: 7px 14px; color: #ddd; cursor: pointer;
            border-radius: 4px; display: flex; align-items: center; gap: 8px;
            transition: background 0.1s;
        }
        #sp-ctx-menu .sp-ctx-item:hover { background: rgba(74,158,255,0.2); color: #fff; }
        #sp-ctx-menu .sp-ctx-sep { border-top: 1px solid rgba(255,255,255,0.1); margin: 3px 0; }
        #sp-ctx-menu .sp-ctx-item.sp-ctx-disabled { color: #666; cursor: default; }
        #sp-ctx-menu .sp-ctx-item.sp-ctx-disabled:hover { background: none; color: #666; }
        /* ë°”ë‹¥ ì—¬ë°± ë³´ì • */
        body { padding-bottom: 48px !important; }
    `;
    document.head.appendChild(style);

    /* â”€â”€ ì»¤ìŠ¤í…€ ì»¨í…ìŠ¤íŠ¸ ë©”ë‰´ â”€â”€ */
    const ctxMenu = document.createElement('div');
    ctxMenu.id = 'sp-ctx-menu';
    document.body.appendChild(ctxMenu);

    let ctxTargetLink = null;
    let ctxSelectedText = '';

    document.addEventListener('contextmenu', (e) => {
        e.preventDefault();
        /* íƒ€ê²Ÿ ë¶„ì„ */
        const link = e.target.closest('a[href]');
        ctxTargetLink = link ? getFullUrl(link.getAttribute('href')) : null;
        ctxSelectedText = window.getSelection().toString().trim();

        let items = '';

        /* ë§í¬ ë©”ë‰´ */
        if (ctxTargetLink) {
            const isVid = isVideoUrl(ctxTargetLink);
            const isAdded = addedUrls.has(ctxTargetLink);
            items += `<div class="sp-ctx-item" data-action="open-tab">ğŸ”— ìƒˆ íƒ­ì—ì„œ ì—´ê¸°</div>`;
            items += `<div class="sp-ctx-item" data-action="copy-link">ğŸ“‹ ë§í¬ ì£¼ì†Œ ë³µì‚¬</div>`;
            if (isVid && !isAdded) {
                items += `<div class="sp-ctx-item" data-action="add-queue">â• ëŒ€ê¸°ì—´ì— ì¶”ê°€</div>`;
            } else if (isVid && isAdded) {
                items += `<div class="sp-ctx-item sp-ctx-disabled">âœ… ì´ë¯¸ ì¶”ê°€ë¨</div>`;
            }
            items += `<div class="sp-ctx-sep"></div>`;
        }

        /* í…ìŠ¤íŠ¸ ì„ íƒ ë©”ë‰´ */
        if (ctxSelectedText) {
            items += `<div class="sp-ctx-item" data-action="copy-text">ğŸ“„ í…ìŠ¤íŠ¸ ë³µì‚¬</div>`;
            items += `<div class="sp-ctx-sep"></div>`;
        }

        /* ê³µí†µ ë©”ë‰´ */
        items += `<div class="sp-ctx-item" data-action="back">â—€ ë’¤ë¡œ</div>`;
        items += `<div class="sp-ctx-item" data-action="forward">â–¶ ì•ìœ¼ë¡œ</div>`;
        items += `<div class="sp-ctx-item" data-action="reload">ğŸ”„ ìƒˆë¡œê³ ì¹¨</div>`;
        items += `<div class="sp-ctx-item" data-action="copy-page-url">ğŸŒ í˜ì´ì§€ URL ë³µì‚¬</div>`;

        ctxMenu.innerHTML = items;
        ctxMenu.style.display = 'block';

        /* ìœ„ì¹˜ ê³„ì‚° (í™”ë©´ ë°– ë°©ì§€) */
        const mw = ctxMenu.offsetWidth, mh = ctxMenu.offsetHeight;
        let x = e.clientX, y = e.clientY;
        if (x + mw > window.innerWidth) x = window.innerWidth - mw - 4;
        if (y + mh > window.innerHeight - 44) y = window.innerHeight - 44 - mh - 4;
        ctxMenu.style.left = x + 'px';
        ctxMenu.style.top = y + 'px';
    });

    document.addEventListener('click', () => { ctxMenu.style.display = 'none'; });
    document.addEventListener('scroll', () => { ctxMenu.style.display = 'none'; }, true);

    ctxMenu.addEventListener('click', async (e) => {
        const item = e.target.closest('.sp-ctx-item');
        if (!item || item.classList.contains('sp-ctx-disabled')) return;
        const action = item.dataset.action;
        ctxMenu.style.display = 'none';

        switch (action) {
            case 'copy-link':
                if (ctxTargetLink) navigator.clipboard.writeText(ctxTargetLink).catch(() => {});
                break;
            case 'copy-text':
                if (ctxSelectedText) navigator.clipboard.writeText(ctxSelectedText).catch(() => {});
                break;
            case 'copy-page-url':
                navigator.clipboard.writeText(location.href).catch(() => {});
                break;
            case 'open-tab':
                if (ctxTargetLink && window.pywebview && window.pywebview.api) {
                    statusEl.textContent = 'ğŸ”— ìƒˆ íƒ­ ì—´ê¸°...';
                    await window.pywebview.api.open_new_tab(ctxTargetLink);
                    setTimeout(() => statusEl.textContent = '', 1500);
                }
                break;
            case 'add-queue':
                if (ctxTargetLink && window.pywebview && window.pywebview.api) {
                    statusEl.textContent = 'â³ ì¶”ê°€ ì¤‘...';
                    try {
                        const res = await window.pywebview.api.add_to_queue(ctxTargetLink);
                        if (res.ok || res.duplicate) {
                            addedUrls.add(ctxTargetLink);
                            statusEl.textContent = 'âœ… ' + (res.title || 'ì¶”ê°€ ì™„ë£Œ');
                            updateCount(); injectCardButtons();
                        } else {
                            statusEl.textContent = 'âŒ ' + (res.error || 'ì‹¤íŒ¨');
                        }
                    } catch { statusEl.textContent = 'âŒ ì˜¤ë¥˜'; }
                    setTimeout(() => statusEl.textContent = '', 3000);
                }
                break;
            case 'back': history.back(); break;
            case 'forward': history.forward(); break;
            case 'reload': location.reload(); break;
        }
    });

    /* â”€â”€ íˆ´ë°” HTML â”€â”€ */
    const toolbar = document.createElement('div');
    toolbar.id = 'sp-toolbar';
    const isVideo = isVideoUrl(location.href);
    toolbar.innerHTML = `
        <span class="sp-brand">â–¶ SP</span>
        <button class="sp-btn ${isVideo ? '' : 'sp-na'}" id="sp-add-btn"
                ${isVideo ? '' : 'disabled'} title="${isVideo ? 'ì´ ì˜ìƒì„ ëŒ€ê¸°ì—´ì— ì¶”ê°€' : 'ì˜ìƒ í˜ì´ì§€ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥'}">
            ${isVideo ? '+ ì¶”ê°€' : 'ì˜ìƒ ì•„ë‹˜'}
        </button>
        <button class="sp-icon-btn" id="sp-reinject-btn" title="[+] ë²„íŠ¼ ê°•ì œ í‘œì‹œ">ğŸ”§</button>
        <button class="sp-icon-btn" id="sp-refresh-btn" title="ìƒˆë¡œê³ ì¹¨">â†»</button>
        <button class="sp-icon-btn" id="sp-newtab-btn" title="í˜„ì¬ í˜ì´ì§€ë¥¼ ìƒˆ íƒ­ìœ¼ë¡œ">â§‰</button>
        <span class="sp-status" id="sp-status"></span>
        <span class="sp-count" id="sp-count"></span>
    `;
    document.body.appendChild(toolbar);

    const addBtn = document.getElementById('sp-add-btn');
    const statusEl = document.getElementById('sp-status');
    const countEl = document.getElementById('sp-count');

    /* â”€â”€ ê°•ì œ ì¸ì ì…˜ ë²„íŠ¼ â”€â”€ */
    document.getElementById('sp-reinject-btn').addEventListener('click', () => {
        document.querySelectorAll('[data-sp-done]').forEach(el => el.removeAttribute('data-sp-done'));
        injectCardButtons();
        statusEl.textContent = 'ğŸ”§ [+] ë²„íŠ¼ ê°•ì œ í‘œì‹œ ì™„ë£Œ';
        setTimeout(() => statusEl.textContent = '', 2000);
    });

    /* â”€â”€ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ â”€â”€ */
    document.getElementById('sp-refresh-btn').addEventListener('click', () => {
        location.reload();
    });

    /* â”€â”€ ìƒˆ íƒ­ ë²„íŠ¼ (í˜„ì¬ í˜ì´ì§€) â”€â”€ */
    document.getElementById('sp-newtab-btn').addEventListener('click', async () => {
        if (window.pywebview && window.pywebview.api) {
            statusEl.textContent = 'ğŸ”— ìƒˆ íƒ­ ì—´ê¸°...';
            await window.pywebview.api.open_new_tab(location.href);
            setTimeout(() => statusEl.textContent = '', 1500);
        }
    });

    /* â”€â”€ ëŒ€ê¸°ì—´ ì¶”ê°€ (í˜„ì¬ í˜ì´ì§€) â”€â”€ */
    if (isVideo) {
        addBtn.addEventListener('click', async () => {
            const url = location.href.split('?')[0];
            if (addedUrls.has(url)) return;
            addBtn.disabled = true;
            addBtn.textContent = 'â³ ì¶”ê°€ ì¤‘...';
            statusEl.textContent = 'ì˜ìƒ ì •ë³´ ì¶”ì¶œ ì¤‘... (ìµœëŒ€ 30ì´ˆ ì†Œìš”)';
            try {
                const res = await window.pywebview.api.add_to_queue(url);
                if (res.error) {
                    if (res.duplicate) {
                        addedUrls.add(url);
                        addBtn.textContent = 'âœ… ì¶”ê°€ë¨';
                        addBtn.classList.add('sp-dup');
                        statusEl.textContent = 'ì´ë¯¸ ëŒ€ê¸°ì—´ì— ìˆìŠµë‹ˆë‹¤.';
                    } else {
                        addBtn.textContent = 'âŒ ì‹¤íŒ¨';
                        statusEl.textContent = 'âŒ ' + res.error;
                        /* ì‹¤íŒ¨ ì‹œ ë²„íŠ¼ ë³µêµ¬í•˜ì—¬ ì¬ì‹œë„ ê°€ëŠ¥ */
                        setTimeout(() => { addBtn.textContent = '+ ì¶”ê°€'; addBtn.disabled = false; }, 5000);
                        setTimeout(() => { statusEl.textContent = ''; }, 8000);
                        return;
                    }
                } else {
                    addedUrls.add(url);
                    addBtn.textContent = 'âœ… ì¶”ê°€ë¨';
                    addBtn.classList.add('sp-added');
                    statusEl.textContent = 'âœ… ' + (res.title || 'ì¶”ê°€ ì™„ë£Œ');
                    updateCount();
                }
                setTimeout(() => { statusEl.textContent = ''; }, 3000);
            } catch(e) {
                addBtn.textContent = 'âŒ ì˜¤ë¥˜';
                statusEl.textContent = 'âŒ ì˜¤ë¥˜: ' + (e.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜');
                setTimeout(() => { addBtn.textContent = '+ ì¶”ê°€'; addBtn.disabled = false; statusEl.textContent = ''; }, 5000);
            }
        });
    }

    /* â”€â”€ ì¹´ë“œ ì˜¤ë²„ë ˆì´ ë²„íŠ¼ ì¸ì ì…˜ â”€â”€ */
    function injectCardButtons() {
        document.querySelectorAll('a[href]').forEach(a => {
            if (a.dataset.spDone) return;
            a.dataset.spDone = '1';
            const href = a.getAttribute('href');
            if (!href || href.startsWith('#') || href.startsWith('javascript')) return;
            const fullUrl = getFullUrl(href);
            if (!fullUrl || !isVideoUrl(fullUrl)) return;
            const img = a.querySelector('img');
            if (!img) return;
            const wrap = a.closest('div') || a;
            const cs = getComputedStyle(wrap);
            if (cs.position === 'static') wrap.style.position = 'relative';

            const btn = document.createElement('button');
            btn.className = 'sp-card-btn' + (addedUrls.has(fullUrl) ? ' sp-card-added' : '');
            btn.textContent = addedUrls.has(fullUrl) ? 'âœ“' : '+';
            btn.title = addedUrls.has(fullUrl) ? 'ì¶”ê°€ë¨' : 'ëŒ€ê¸°ì—´ì— ì¶”ê°€';

            btn.addEventListener('click', async (e) => {
                e.preventDefault(); e.stopPropagation();
                if (addedUrls.has(fullUrl)) return;
                btn.disabled = true; btn.textContent = 'â€¦';
                statusEl.textContent = 'â³ ì¶”ê°€ ì¤‘...';
                try {
                    const res = await window.pywebview.api.add_to_queue(fullUrl);
                    if (res.error) {
                        if (res.duplicate) {
                            addedUrls.add(fullUrl); btn.textContent = 'âœ“'; btn.classList.add('sp-card-added');
                            statusEl.textContent = 'ì´ë¯¸ ëŒ€ê¸°ì—´ì— ìˆìŠµë‹ˆë‹¤.';
                        } else {
                            btn.textContent = '!';
                            statusEl.textContent = 'âŒ ' + res.error;
                            /* ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡ ë³µêµ¬ */
                            setTimeout(() => { btn.textContent = '+'; btn.disabled = false; }, 5000);
                            setTimeout(() => { statusEl.textContent = ''; }, 8000);
                            return;
                        }
                    } else {
                        addedUrls.add(fullUrl);
                        btn.textContent = 'âœ“'; btn.classList.add('sp-card-added');
                        statusEl.textContent = 'âœ… ' + (res.title || 'ì¶”ê°€ ì™„ë£Œ');
                        updateCount();
                    }
                    setTimeout(() => { statusEl.textContent = ''; }, 3000);
                } catch(e) {
                    btn.textContent = '!';
                    statusEl.textContent = 'âŒ ì˜¤ë¥˜: ' + (e.message || 'ì¶”ê°€ ì‹¤íŒ¨');
                    setTimeout(() => { btn.textContent = '+'; btn.disabled = false; }, 5000);
                    setTimeout(() => { statusEl.textContent = ''; }, 8000);
                }
            });
            wrap.appendChild(btn);
        });
    }

    function updateCount() {
        queueCount = addedUrls.size;
        countEl.textContent = 'ëŒ€ê¸°ì—´: ' + queueCount + 'ê°œ';
    }

    /* â”€â”€ Ctrl+í´ë¦­, ì¤‘ê°„ë²„íŠ¼ í´ë¦­ â†’ ìƒˆ íƒ­ â”€â”€ */
    document.addEventListener('click', (e) => {
        if (!e.ctrlKey && e.button !== 1) return;
        const link = e.target.closest('a[href]');
        if (!link) return;
        const href = getFullUrl(link.getAttribute('href'));
        if (!href) return;
        e.preventDefault(); e.stopPropagation();
        if (window.pywebview && window.pywebview.api) {
            window.pywebview.api.open_new_tab(href);
        }
    }, true);
    document.addEventListener('auxclick', (e) => {
        if (e.button !== 1) return;
        const link = e.target.closest('a[href]');
        if (!link) return;
        const href = getFullUrl(link.getAttribute('href'));
        if (!href) return;
        e.preventDefault(); e.stopPropagation();
        if (window.pywebview && window.pywebview.api) {
            window.pywebview.api.open_new_tab(href);
        }
    }, true);

    /* â”€â”€ ì´ˆê¸°í™” â”€â”€ */
    async function init() {
        try {
            const urls = await window.pywebview.api.get_queue_urls();
            urls.forEach(u => addedUrls.add(u));
            urls.forEach(u => addedUrls.add(u.split('?')[0]));
            queueCount = urls.length;
            updateCount();
            if (isVideo && addedUrls.has(location.href.split('?')[0])) {
                addBtn.textContent = 'âœ… ì¶”ê°€ë¨';
                addBtn.classList.add('sp-dup');
                addBtn.disabled = true;
            }
        } catch(e) { console.warn('[SP] ëŒ€ê¸°ì—´ ë¡œë“œ ì‹¤íŒ¨:', e); }
        injectCardButtons();
        /* MutationObserver: ë””ë°”ìš´ìŠ¤ ì ìš© */
        let mutTimer = null;
        const observer = new MutationObserver(() => {
            if (mutTimer) return;
            mutTimer = setTimeout(() => { mutTimer = null; injectCardButtons(); }, 300);
        });
        observer.observe(document.body, { childList: true, subtree: true });
        /* ì•ˆì „ë§: 3ì´ˆ ê°„ê²© ì£¼ê¸°ì  ì¬ê²€ì‚¬ */
        setInterval(injectCardButtons, 3000);
    }

    /* pywebview APIê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° */
    function waitForApi() {
        if (window.pywebview && window.pywebview.api) { init(); }
        else { setTimeout(waitForApi, 200); }
    }
    waitForApi();
})();
"""

def set_webview_ready():
    """pywebview guilib ì´ˆê¸°í™” ì™„ë£Œ ì‹œê·¸ë„. app.pyì˜ func ì½œë°±ì—ì„œ í˜¸ì¶œ."""
    _webview_ready.set()
    print("  [pywebview] GUI ì´ˆê¸°í™” ì™„ë£Œ â€” ê²€ìƒ‰ ì°½ ë™ì  ìƒì„± ê°€ëŠ¥")


def _open_browse_tab(url):
    """ìƒˆ pywebview ì°½(íƒ­)ì„ ì—´ì–´ ì§€ì • URLì˜ MissAV ì‚¬ì´íŠ¸ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    ê¸°ì¡´ íƒìƒ‰ ì°½ê³¼ ë™ì¼í•œ js_apiì™€ JS ì¸ì ì…˜ì„ ì ìš©í•©ë‹ˆë‹¤."""
    import webview
    import time as _time

    def _inject_tab_js(win):
        """ìƒˆ íƒ­ JS ì¸ì ì…˜ (ì§€ì—° + ì¬ì‹œë„)"""
        _time.sleep(1.5)
        for attempt in range(5):
            try:
                win.evaluate_js(_BROWSE_INJECT_JS)
                print(f"  [ìƒˆíƒ­] JS ì¸ì ì…˜ ì„±ê³µ (ì‹œë„ {attempt+1})")
                return
            except Exception as e:
                print(f"  [ìƒˆíƒ­] JS ì¸ì ì…˜ ì‹œë„ {attempt+1}/5 ì‹¤íŒ¨: {e}")
                if attempt < 4:
                    _time.sleep(1.0)
        print(f"  [ìƒˆíƒ­] JS ì¸ì ì…˜ ìµœì¢… ì‹¤íŒ¨")

    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        path_slug = parsed.path.rstrip('/').split('/')[-1][:40] if parsed.path else ''
        title = f"MissAV â€” {path_slug}" if path_slug else "MissAV â€” ìƒˆ íƒ­"

        new_win = webview.create_window(
            title=title,
            url=url,
            width=1100,
            height=800,
            min_size=(700, 500),
            text_select=True,
            js_api=_browse_api,
        )
        if new_win is not None:
            # loaded ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ (í˜ì´ì§€ ì´ë™ë§ˆë‹¤)
            new_win.events.loaded += lambda: threading.Thread(
                target=_inject_tab_js, args=(new_win,), daemon=True
            ).start()
            # â˜… ìµœì´ˆ ë¡œë“œ ëˆ„ë½ ëŒ€ë¹„ ìˆ˜ë™ ì¸ì ì…˜
            threading.Thread(
                target=_inject_tab_js, args=(new_win,), daemon=True
            ).start()
            print(f"  [ìƒˆíƒ­] ì—´ë¦¼: {url[:80]}")
        else:
            print(f"  [ìƒˆíƒ­] ì°½ ìƒì„± ì‹¤íŒ¨: {url[:80]}")
    except Exception as e:
        print(f"  [ìƒˆíƒ­] ì˜¤ë¥˜: {e}")


def _is_cf_blocked(html: str) -> bool:
    """Cloudflare ì°¨ë‹¨ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    check = html[:5000].lower()
    cf_signs = [
        'just a moment', 'checking your browser', 'cf-turnstile',
        'challenge-platform', 'cf-browser-verification', 'verify you are human',
        '_cf_chl_opt', 'window._cf_chl_opt',
    ]
    hits = sum(1 for s in cf_signs if s in check)
    return hits >= 2


def _detect_browser() -> str:
    """ì„¤ì¹˜ëœ ë¸Œë¼ìš°ì €ë¥¼ ìë™ ê°ì§€í•©ë‹ˆë‹¤. (yt-dlpê°€ ì§€ì›í•˜ëŠ” ì´ë¦„ ë°˜í™˜)"""
    import shutil
    # ìœˆë„ìš°ì—ì„œ ì‹¤ì œ ê²½ë¡œ í™•ì¸
    browser_checks = [
        ('edge', [
            os.path.expandvars(r'%ProgramFiles(x86)%\Microsoft\Edge\Application\msedge.exe'),
            os.path.expandvars(r'%ProgramFiles%\Microsoft\Edge\Application\msedge.exe'),
        ]),
        ('chrome', [
            os.path.expandvars(r'%ProgramFiles%\Google\Chrome\Application\chrome.exe'),
            os.path.expandvars(r'%ProgramFiles(x86)%\Google\Chrome\Application\chrome.exe'),
            os.path.expandvars(r'%LocalAppData%\Google\Chrome\Application\chrome.exe'),
        ]),
        ('brave', [
            os.path.expandvars(r'%ProgramFiles%\BraveSoftware\Brave-Browser\Application\brave.exe'),
            os.path.expandvars(r'%LocalAppData%\BraveSoftware\Brave-Browser\Application\brave.exe'),
        ]),
        ('firefox', [
            os.path.expandvars(r'%ProgramFiles%\Mozilla Firefox\firefox.exe'),
            os.path.expandvars(r'%ProgramFiles(x86)%\Mozilla Firefox\firefox.exe'),
        ]),
        ('opera', [
            os.path.expandvars(r'%LocalAppData%\Programs\Opera\opera.exe'),
            os.path.expandvars(r'%AppData%\Opera Software\Opera Stable\opera.exe'),
        ]),
    ]

    for name, paths in browser_checks:
        for p in paths:
            if os.path.exists(p):
                print(f"[ë¸Œë¼ìš°ì € ê°ì§€] {name} ë°œê²¬: {p}")
                return name
        # shutil.which í´ë°± (PATHì— ìˆëŠ” ê²½ìš°)
        exe_name = name if name != 'edge' else 'msedge'
        if shutil.which(exe_name):
            print(f"[ë¸Œë¼ìš°ì € ê°ì§€] {name} (PATH)")
            return name

    # Whaleì€ Chromium ê¸°ë°˜ â†’ Edgeì˜ ì¿ í‚¤ ê²½ë¡œì™€ ìœ ì‚¬í•œ êµ¬ì¡°
    # yt-dlpëŠ” whaleì„ ëª¨ë¥´ì§€ë§Œ, Chromium ê¸°ë°˜ ì¿ í‚¤ DB íŒŒì¼ì„ ì§ì ‘ ì½ì„ ìˆ˜ ìˆìŒ
    whale_paths = [
        os.path.expandvars(r'%LocalAppData%\Naver\Naver Whale\User Data'),
    ]
    for wp in whale_paths:
        if os.path.isdir(wp):
            print(f"[ë¸Œë¼ìš°ì € ê°ì§€] Whale ë°œê²¬ (Chromium í˜¸í™˜ ëª¨ë“œ ì‚¬ìš©)")
            # Whaleì€ chromiumê³¼ ê²½ë¡œ êµ¬ì¡°ê°€ ê°™ìŒ
            return 'chromium'

    print("[ë¸Œë¼ìš°ì € ê°ì§€] ë¸Œë¼ìš°ì €ë¥¼ ì°¾ì§€ ëª»í•¨")
    return ''


_detected_browser = None  # ìºì‹œ

def _get_browser():
    """ê°ì§€ëœ ë¸Œë¼ìš°ì €ë¥¼ ìºì‹œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _detected_browser
    if _detected_browser is None:
        _detected_browser = _detect_browser()
    return _detected_browser


def _build_cookie_jar_from_browser(browser: str = ''):
    """yt-dlpë¥¼ í†µí•´ ë¸Œë¼ìš°ì €ì˜ ì¿ í‚¤ jarë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not browser:
        browser = _get_browser()
    if not browser:
        return None, ''

    try:
        # yt-dlp YoutubeDL ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ì–´ ì¿ í‚¤ jarë§Œ ê°€ì ¸ì˜¤ê¸°
        # ì´ ë°©ì‹ì´ ë‚´ë¶€ APIë³´ë‹¤ ì•ˆì •ì 
        opts = {
            "quiet": True,
            "no_warnings": True,
            "skip_download": True,
            "cookiesfrombrowser": (browser,),
        }
        ydl = yt_dlp.YoutubeDL(opts)
        cookie_jar = ydl.cookiejar
        print(f"[ì¿ í‚¤] {browser}ì—ì„œ ì¿ í‚¤ jar ì¶”ì¶œ ì„±ê³µ ({len(cookie_jar)}ê°œ)")
        return cookie_jar, browser
    except Exception as e:
        print(f"[ì¿ í‚¤] {browser}ì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        # ë‹¤ë¥¸ ë¸Œë¼ìš°ì € ì‹œë„
        for alt in ['edge', 'chrome', 'chromium', 'firefox', 'brave']:
            if alt == browser:
                continue
            try:
                opts = {
                    "quiet": True,
                    "no_warnings": True,
                    "skip_download": True,
                    "cookiesfrombrowser": (alt,),
                }
                ydl = yt_dlp.YoutubeDL(opts)
                cookie_jar = ydl.cookiejar
                print(f"[ì¿ í‚¤] {alt}ì—ì„œ ì¿ í‚¤ jar ì¶”ì¶œ ì„±ê³µ ({len(cookie_jar)}ê°œ)")
                return cookie_jar, alt
            except Exception:
                continue
    return None, ''


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ì €ì¥/ë¡œë“œ (íŒŒì¼ ê¸°ë°˜, ìš©ëŸ‰ ë¬´ì œí•œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_data_lock = threading.Lock()

def _load_data():
    """data.jsonì„ ë¡œë“œí•©ë‹ˆë‹¤. ì†ìƒ ì‹œ ë°±ì—…ì—ì„œ ë³µêµ¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤."""
    for fp in [DATA_FILE, Path(str(DATA_FILE) + ".bak"), Path(str(DATA_FILE) + ".bak2")]:
        if not fp.exists():
            continue
        try:
            with open(fp, "r", encoding="utf-8") as f:
                data = json.load(f)
            if "categories" not in data:
                data["categories"] = []
            if fp != DATA_FILE:
                print(f"  [ë³µêµ¬] {fp.name}ì—ì„œ ë°ì´í„° ë³µêµ¬ ì„±ê³µ")
                # ë³µêµ¬ëœ ë°ì´í„°ë¥¼ ì›ë³¸ì— ì €ì¥
                try:
                    with open(DATA_FILE, "w", encoding="utf-8") as f2:
                        json.dump(data, f2, ensure_ascii=False, indent=2)
                except Exception:
                    pass
            return data
        except (json.JSONDecodeError, ValueError) as e:
            print(f"  [ê²½ê³ ] {fp.name} ì†ìƒë¨: {e}")
            continue
        except Exception:
            continue
    return {"queue": [], "playback": {}, "heatmaps": {}, "categories": []}

def _save_data(data):
    """data.jsonì„ ì €ì¥í•©ë‹ˆë‹¤. ì €ì¥ ì „ ì´ì¤‘ ë°±ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ë°±ì—… ìˆœí™˜: data.json.bak â†’ data.json.bak2, data.json â†’ data.json.bak
    â˜… ë°ì´í„° ê¸‰ê° ê°ì§€ ì‹œ .safety ë°±ì—… ìƒì„± (ë®ì–´ì“°ê¸° ë°©ì§€)"""
    with _data_lock:
        bak = Path(str(DATA_FILE) + ".bak")
        bak2 = Path(str(DATA_FILE) + ".bak2")
        safety = Path(str(DATA_FILE) + ".safety")

        # â˜… ë°ì´í„° ê¸‰ê° ë³´í˜¸: ê¸°ì¡´ ëŒ€ë¹„ ëŒ€ê¸°ì—´ì´ 50% ì´ìƒ ì¤„ì—ˆìœ¼ë©´ .safety ë°±ì—…
        if DATA_FILE.exists():
            try:
                with open(DATA_FILE, "r", encoding="utf-8") as f:
                    existing = json.load(f)
                old_count = len(existing.get("queue", []))
                new_count = len(data.get("queue", []))
                if old_count > 10 and new_count < old_count * 0.5:
                    # .safetyê°€ ì—†ê±°ë‚˜ .safetyë³´ë‹¤ ê¸°ì¡´ dataê°€ ë” í¬ë©´ ë°±ì—…
                    save_safety = True
                    if safety.exists():
                        try:
                            with open(safety, "r", encoding="utf-8") as sf:
                                safety_data = json.load(sf)
                            if len(safety_data.get("queue", [])) >= old_count:
                                save_safety = False  # ì´ë¯¸ ë” í° safety ë°±ì—… ìˆìŒ
                        except:
                            pass
                    if save_safety:
                        import shutil
                        shutil.copy2(str(DATA_FILE), str(safety))
                        print(f"  [âš  ì•ˆì „ë°±ì—…] ëŒ€ê¸°ì—´ ê¸‰ê° ê°ì§€! ({old_count}â†’{new_count}) .safety ë°±ì—… ìƒì„±")
            except:
                pass

        try:
            # ë°±ì—… ìˆœí™˜: .bak â†’ .bak2
            if bak.exists():
                import shutil
                shutil.copy2(str(bak), str(bak2))
            # í˜„ì¬ data.json â†’ .bak
            if DATA_FILE.exists():
                import shutil
                shutil.copy2(str(DATA_FILE), str(bak))
        except Exception as e:
            print(f"  [ë°±ì—…] íšŒì „ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        # ì•ˆì „ ì“°ê¸°: ì„ì‹œ íŒŒì¼ì— ë¨¼ì € ì“°ê³  rename
        tmp = Path(str(DATA_FILE) + ".tmp")
        try:
            with open(tmp, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            # Windowsì—ì„œ rename ì „ì— ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í•„ìš”
            if DATA_FILE.exists():
                DATA_FILE.unlink()
            tmp.rename(DATA_FILE)
        except Exception as e:
            print(f"  [ì €ì¥] ì•ˆì „ ì“°ê¸° ì‹¤íŒ¨, ì§ì ‘ ì“°ê¸° ì‹œë„: {e}")
            try:
                with open(DATA_FILE, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            except Exception as e2:
                print(f"  [ì €ì¥] ì§ì ‘ ì“°ê¸°ë„ ì‹¤íŒ¨: {e2}")

def _url_id(url: str) -> str:
    return hashlib.md5(url.encode()).hexdigest()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì • ê´€ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_SETTINGS = {
    "quality": "best",
    "downloadFolder": "",
    "skipForward": 10,
    "skipBackward": 10,
    "skipForwardShift": 5,
    "skipBackwardShift": 5,
    "defaultVolume": 1.0,
    "defaultSpeed": 1.0,
    "autoplayNext": True,
    "alwaysOnTop": False,
    "windowWidth": 1400,
    "windowHeight": 850,
}

def _load_settings():
    data = _load_data()
    saved = data.get("settings", {})
    return {**DEFAULT_SETTINGS, **saved}

def _save_settings(settings):
    data = _load_data()
    data["settings"] = settings
    _save_data(data)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# yt-dlp í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _ydl_opts(extract_only=True):
    browser = _get_browser()
    opts = {
        "quiet": True,
        "no_warnings": True,
        "format": "best",
        "noplaylist": True,
        # Cloudflare ìš°íšŒ: ë¸Œë¼ìš°ì € í”ë‚´
        "impersonate": "chrome",
        "extractor_args": {"generic": {"impersonate": ["true"]}},
    }
    # ë¸Œë¼ìš°ì € ì¿ í‚¤ ìë™ ì¶”ì¶œ (ìš°ì„ )
    if browser:
        opts["cookiesfrombrowser"] = (browser,)
    # cookies.txt í´ë°±
    if COOKIES_FILE.exists():
        if not browser:
            opts["cookiefile"] = str(COOKIES_FILE)
    if extract_only:
        opts["skip_download"] = True
    return opts

# ì¶”ì¶œ ê²°ê³¼ ìºì‹œ (ê°™ì€ ì˜ìƒ ì¬ìƒ ì‹œ ì¦‰ì‹œ ì‹œì‘)
_extract_cache = {}  # url -> {"info": ..., "time": ...}
_CACHE_TTL = 3600  # 1ì‹œê°„

# M3U8 ì»¨í…ì¸  ìºì‹œ (ì²˜ë¦¬ëœ M3U8ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥)
_m3u8_content_cache = {}  # video_url -> {"content": str, "time": float}
_M3U8_CONTENT_TTL = 1800  # 30ë¶„

def _fetch_and_cache_m3u8(video_url, headers):
    """
    M3U8ë¥¼ CDNì—ì„œ ê°€ì ¸ì™€ì„œ ì²˜ë¦¬(ìƒëŒ€â†’ì ˆëŒ€ URL)í•˜ê³  ìºì‹œí•©ë‹ˆë‹¤.
    ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ìºì‹œ í™•ì¸
    if video_url in _m3u8_content_cache:
        cached = _m3u8_content_cache[video_url]
        if time.time() - cached["time"] < _M3U8_CONTENT_TTL:
            return cached["content"]

    resp = requests.get(video_url, headers=headers, timeout=15)
    resp.raise_for_status()

    content = resp.text
    base_url = video_url.rsplit('/', 1)[0] + '/'
    lines = content.split('\n')
    fixed_lines = []
    for line in lines:
        line = line.strip()
        if line and not line.startswith('#'):
            if not line.startswith('http'):
                line = base_url + line
        fixed_lines.append(line)

    result = '\n'.join(fixed_lines)
    _m3u8_content_cache[video_url] = {"content": result, "time": time.time()}
    return result

def _background_preextract():
    """ì„œë²„ ì‹œì‘ ì‹œ stream_urlì´ ì—†ëŠ” ëŒ€ê¸°ì—´ í•­ëª©ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    time.sleep(3)  # ì„œë²„ ì‹œì‘ ëŒ€ê¸°
    try:
        data = _load_data()
        urls_to_extract = []
        for item in data.get("queue", []):
            if not item.get("stream_url"):
                urls_to_extract.append({"url": item["url"], "title": item.get("title", "?"), "id": item["id"]})
        if not urls_to_extract:
            print("[ì‚¬ì „ ì¶”ì¶œ] ëª¨ë“  í•­ëª©ì´ ì¤€ë¹„ë¨")
            return
        print(f"[ì‚¬ì „ ì¶”ì¶œ] {len(urls_to_extract)}ê°œ í•­ëª© ì²˜ë¦¬ ì‹œì‘...")
        success = 0
        for item_ref in urls_to_extract:
            try:
                info = _extract_info(item_ref["url"])
                video_url = info.get("url", "")
                if video_url:
                    # â˜… ë§¤ë²ˆ ìµœì‹  ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•´ì„œ ì €ì¥ (race condition ë°©ì§€)
                    fresh_data = _load_data()
                    for q in fresh_data.get("queue", []):
                        if q["id"] == item_ref["id"]:
                            q["stream_url"] = video_url
                            q["http_headers"] = info.get("http_headers", {})
                            q["variants"] = info.get("_variants", [])
                            q["_extracted_at"] = time.time()
                            break
                    _save_data(fresh_data)
                    # M3U8 ë„ ë¯¸ë¦¬ ìºì‹œ
                    headers = {'User-Agent': USER_AGENT}
                    headers.update(info.get("http_headers", {}))
                    if '.m3u8' in video_url:
                        try:
                            _fetch_and_cache_m3u8(video_url, headers)
                        except:
                            pass
                    success += 1
                    print(f"[ì‚¬ì „ ì¶”ì¶œ] âœ“ {item_ref['title'][:40]}")
            except Exception as e:
                print(f"[ì‚¬ì „ ì¶”ì¶œ] âœ— {item_ref['title'][:40]}: {e}")
            time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        print(f"[ì‚¬ì „ ì¶”ì¶œ] ì™„ë£Œ ({success}/{len(urls_to_extract)} ì„±ê³µ)")
    except Exception as e:
        print(f"[ì‚¬ì „ ì¶”ì¶œ] ì˜¤ë¥˜: {e}")

def _extract_info(url: str, use_cache=True):
    """URLì—ì„œ ì˜ìƒ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ì»¤ìŠ¤í…€ ë„ë©”ì¸ì€ ë°”ë¡œ ì»¤ìŠ¤í…€ ì¶”ì¶œê¸° ì‚¬ìš©."""
    # ìºì‹œ í™•ì¸
    if use_cache and url in _extract_cache:
        cached = _extract_cache[url]
        if time.time() - cached["time"] < _CACHE_TTL:
            print(f"[ìºì‹œ] ì¶”ì¶œ ê²°ê³¼ ìºì‹œ ì‚¬ìš© ({url[:60]}...)")
            return cached["info"]

    parsed = urllib.parse.urlparse(url)
    is_custom = any(d in parsed.netloc for d in CUSTOM_DOMAINS)

    # ì»¤ìŠ¤í…€ ë„ë©”ì¸ì€ yt-dlp ê±´ë„ˆë›°ê¸° (2~3ë¶„ ëŒ€ê¸° ë°©ì§€ â†’ ì¦‰ì‹œ ì¶”ì¶œ)
    if is_custom:
        print(f"[ì»¤ìŠ¤í…€ ë„ë©”ì¸] {parsed.netloc} â†’ ì»¤ìŠ¤í…€ ì¶”ì¶œê¸° ë°”ë¡œ ì‚¬ìš©")
        info = _custom_extract(url)
        _extract_cache[url] = {"info": info, "time": time.time()}
        return info

    # ê·¸ ì™¸ ì‚¬ì´íŠ¸ëŠ” yt-dlp ì‹œë„
    try:
        opts = _ydl_opts(extract_only=True)
        with yt_dlp.YoutubeDL(opts) as ydl:
            info = ydl.extract_info(url, download=False)
        if info and (info.get("url") or info.get("formats")):
            _extract_cache[url] = {"info": info, "time": time.time()}
            return info
    except Exception as e:
        raise
    raise ValueError("ì˜ìƒ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def _load_cookies_into_session(session):
    """ì¿ í‚¤ íŒŒì¼ì„ ì„¸ì…˜ì— ë¡œë“œí•©ë‹ˆë‹¤."""
    if COOKIES_FILE.exists():
        try:
            from http.cookiejar import MozillaCookieJar
            cj = MozillaCookieJar(str(COOKIES_FILE))
            cj.load(ignore_discard=True, ignore_expires=True)
            session.cookies = cj
            print(f"[ì¿ í‚¤] {len(cj)} ê°œ ì¿ í‚¤ ë¡œë“œë¨")
        except Exception as e:
            print(f"[ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨] {e}")


def _fetch_page_with_cf_bypass(url: str):
    """Cloudflare ìš°íšŒí•˜ì—¬ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ìˆœì„œ: curl_cffi+ë¸Œë¼ìš°ì €ì¿ í‚¤ â†’ curl_cffi+cookies.txt â†’ requests"""
    parsed = urllib.parse.urlparse(url)
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': f'{parsed.scheme}://{parsed.netloc}/',
        'Sec-Ch-Ua': '"Google Chrome";v="125", "Chromium";v="125", "Not.A/Brand";v="24"',
        'Sec-Ch-Ua-Mobile': '?0',
        'Sec-Ch-Ua-Platform': '"Windows"',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Upgrade-Insecure-Requests': '1',
    }

    # â”€â”€ ë°©ë²• 1: curl_cffi + ë¸Œë¼ìš°ì € ì¿ í‚¤ (yt-dlp cookie jar) â”€â”€
    cookie_jar, browser_name = _build_cookie_jar_from_browser()
    if cookie_jar:
        try:
            from curl_cffi import requests as cf_requests
            session = cf_requests.Session(impersonate="chrome")
            # cookie jarì˜ ëª¨ë“  ì¿ í‚¤ë¥¼ ì„¸ì…˜ì— ì¶”ê°€
            domain = parsed.netloc
            parts = domain.split('.')
            base = '.'.join(parts[-2:]) if len(parts) >= 2 else domain
            loaded = 0
            has_cf = False
            for cookie in cookie_jar:
                cd = cookie.domain.lstrip('.')
                if base in cd or cd in domain:
                    session.cookies.set(cookie.name, cookie.value, domain=cookie.domain)
                    loaded += 1
                    if cookie.name == 'cf_clearance':
                        has_cf = True
            print(f"[ë°©ë²•1] {browser_name}ì—ì„œ {loaded}ê°œ ì¿ í‚¤ ë¡œë“œ (cf_clearance: {'âœ“' if has_cf else 'âœ—'})")
            if loaded > 0:
                resp = session.get(url, headers=headers, timeout=30)
                if resp.status_code == 200 and not _is_cf_blocked(resp.text):
                    print(f"[ë°©ë²•1] curl_cffi + {browser_name} ì¿ í‚¤ë¡œ ì„±ê³µ!")
                    return resp.text, session, f'curl_cffi+{browser_name}'
                else:
                    print(f"[ë°©ë²•1] ë¸Œë¼ìš°ì € ì¿ í‚¤ë¡œë„ CF ì°¨ë‹¨ë¨ (cf_clearance={has_cf})")
        except ImportError:
            print("[ë°©ë²•1] curl_cffi ë¯¸ì„¤ì¹˜")
        except Exception as e:
            print(f"[ë°©ë²•1 ì‹¤íŒ¨] {e}")

    # â”€â”€ ë°©ë²• 2: curl_cffi + cookies.txt â”€â”€
    if COOKIES_FILE.exists():
        try:
            from curl_cffi import requests as cf_requests
            session = cf_requests.Session(impersonate="chrome")
            from http.cookiejar import MozillaCookieJar
            cj = MozillaCookieJar(str(COOKIES_FILE))
            cj.load(ignore_discard=True, ignore_expires=True)
            for cookie in cj:
                session.cookies.set(cookie.name, cookie.value, domain=cookie.domain)
            print(f"[ë°©ë²•2] cookies.txtì—ì„œ {len(cj)}ê°œ ì¿ í‚¤ ë¡œë“œ")
            resp = session.get(url, headers=headers, timeout=30)
            if resp.status_code == 200 and not _is_cf_blocked(resp.text):
                print(f"[ë°©ë²•2] curl_cffi + cookies.txtë¡œ ì„±ê³µ!")
                return resp.text, session, 'curl_cffi+cookies.txt'
            else:
                print(f"[ë°©ë²•2] cookies.txtë¡œë„ CF ì°¨ë‹¨ë¨")
        except ImportError:
            print("[ë°©ë²•2] curl_cffi ë¯¸ì„¤ì¹˜")
        except Exception as e:
            print(f"[ë°©ë²•2 ì‹¤íŒ¨] {e}")

    # â”€â”€ ë°©ë²• 3: ì¼ë°˜ requests (ìµœí›„ í´ë°±) â”€â”€
    session = requests.Session()
    session.headers.update({'User-Agent': USER_AGENT})
    _load_cookies_into_session(session)
    resp = session.get(url, headers={**headers, 'User-Agent': USER_AGENT}, timeout=30)
    resp.raise_for_status()
    print(f"[ë°©ë²•3] requests í´ë°± (CF ì°¨ë‹¨ ê°€ëŠ¥ì„± ë†’ìŒ)")
    return resp.text, session, 'requests(í´ë°±)'


# â”€â”€ P.A.C.K.E.R. ë””ì½”ë”© í—¬í¼ í•¨ìˆ˜ë“¤ â”€â”€

def _base_n_decode(token: str, base: int) -> int:
    """P.A.C.K.E.R.ì˜ base-N ì¸ì½”ë”©ëœ í† í°ì„ ì •ìˆ˜ë¡œ ë””ì½”ë”©í•©ë‹ˆë‹¤."""
    ALPHABET = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
    result = 0
    for ch in token:
        idx = ALPHABET.index(ch) if ch in ALPHABET else -1
        if idx < 0 or idx >= base:
            return -1  # ë””ì½”ë”© ë¶ˆê°€
        result = result * base + idx
    return result


def _unpack_packer(pcode: str, base_n: int, count: int, kstr: str) -> str:
    """P.A.C.K.E.R. ë‚œë…í™”ë¥¼ ì‹¤ì œë¡œ ì–¸íŒ©í•©ë‹ˆë‹¤.
    pcode ì•ˆì˜ base-N ì¸ì½”ë”©ëœ í† í°ì„ keywords ë°°ì—´ì˜ ê°’ìœ¼ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤."""
    keywords = kstr.split('|')
    print(f"[UNPACK] keywords ìˆ˜: {len(keywords)}, base: {base_n}")

    def replacer(match):
        token = match.group(0)
        idx = _base_n_decode(token, base_n)
        if 0 <= idx < len(keywords) and keywords[idx]:
            return keywords[idx]
        return token

    # baseì— ë”°ë¥¸ í† í° íŒ¨í„´
    if base_n <= 10:
        pattern = r'\b\d+\b'
    elif base_n <= 36:
        pattern = r'\b[a-zA-Z0-9]+\b'
    else:  # base62
        pattern = r'\b[a-zA-Z0-9]+\b'

    unpacked = re.sub(pattern, replacer, pcode)
    return unpacked


def _reconstruct_m3u8_from_keywords(kstr: str) -> str:
    """hitomi.py ë°©ì‹: í‚¤ì›Œë“œ ë°°ì—´ì˜ ì¸ë±ìŠ¤ íŒ¨í„´ìœ¼ë¡œ M3U8 URLì„ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
    keywords = kstr.split('|')
    print(f"[í‚¤ì›Œë“œì¬êµ¬ì„±] keywords ìˆ˜: {len(keywords)}")
    if len(keywords) < 3:
        return None

    # ë””ë²„ê·¸: í‚¤ì›Œë“œ ëª©ë¡ ì¼ë¶€ ì¶œë ¥
    preview = keywords[:20] if len(keywords) > 20 else keywords
    print(f"[í‚¤ì›Œë“œì¬êµ¬ì„±] í‚¤ì›Œë“œ í”„ë¦¬ë·°: {preview}")

    # hitomi.py ê¸°ë³¸ íŒ¨í„´: protocol=8, domain1=7, domain2=6, path=[5,4,3,2,1], filename=14, extension=0
    patterns = [
        {'name': 'Default(hitomi)', 'protocol_idx': 8, 'domain1_idx': 7, 'domain2_idx': 6,
         'path_indices': [5, 4, 3, 2, 1], 'path_separator': '-',
         'filename_idx': 14, 'extension_idx': 0},
    ]

    # m3u8 í‚¤ì›Œë“œê°€ ìˆëŠ” ì¸ë±ìŠ¤ ì°¾ê¸° â†’ ë™ì  íŒ¨í„´ ìƒì„±
    m3u8_idx = None
    protocol_indices = []
    for i, kw in enumerate(keywords):
        if kw.lower() == 'm3u8':
            m3u8_idx = i
        if kw.lower() in ('https', 'http'):
            protocol_indices.append(i)

    # m3u8 í‚¤ì›Œë“œë¡œ ë™ì  íŒ¨í„´ ìƒì„± ì‹œë„
    if m3u8_idx is not None and protocol_indices:
        print(f"[í‚¤ì›Œë“œì¬êµ¬ì„±] m3u8 ì¸ë±ìŠ¤: {m3u8_idx}, protocol ì¸ë±ìŠ¤: {protocol_indices}")
        for p_idx in protocol_indices:
            # í”„ë¡œí† ì½œê³¼ m3u8 ì‚¬ì´ì˜ í‚¤ì›Œë“œê°€ ë„ë©”ì¸+ê²½ë¡œ+íŒŒì¼ëª…
            if p_idx < m3u8_idx and (m3u8_idx - p_idx) >= 3:
                # í”„ë¡œí† ì½œ ë‹¤ìŒ 2ê°œê°€ ë„ë©”ì¸, ë§ˆì§€ë§‰ì´ íŒŒì¼ëª…, ë‚˜ë¨¸ì§€ê°€ ê²½ë¡œ
                d1_idx = p_idx - 1 if p_idx > 0 else p_idx + 1
                d2_idx = p_idx - 2 if p_idx > 1 else p_idx + 2
                fn_idx = m3u8_idx + 1 if m3u8_idx + 1 < len(keywords) else m3u8_idx - 1

                # ê²½ë¡œ ì¸ë±ìŠ¤ ì¶”ì • (í”„ë¡œí† ì½œê³¼ m3u8 ì‚¬ì´)
                path_start = min(d1_idx, d2_idx) - 1
                path_end = 0
                path_idxs = list(range(path_start, path_end, -1)) if path_start > path_end else []

                patterns.append({
                    'name': f'Dynamic(p={p_idx},m={m3u8_idx})',
                    'protocol_idx': p_idx, 'domain1_idx': d1_idx, 'domain2_idx': d2_idx,
                    'path_indices': path_idxs[:8],  # ìµœëŒ€ 8ì„¸ê·¸ë¨¼íŠ¸
                    'path_separator': '-',
                    'filename_idx': fn_idx, 'extension_idx': m3u8_idx
                })

    for patt in patterns:
        p_name = patt['name']
        try:
            indices = [patt['protocol_idx'], patt['domain1_idx'], patt['domain2_idx'],
                       patt['filename_idx'], patt['extension_idx']] + patt['path_indices']
            if any(idx >= len(keywords) or idx < 0 for idx in indices):
                print(f"[í‚¤ì›Œë“œì¬êµ¬ì„±] {p_name}: ì¸ë±ìŠ¤ ë²”ìœ„ ì´ˆê³¼ (keywords: {len(keywords)})")
                continue

            proto = keywords[patt['protocol_idx']]
            d1 = keywords[patt['domain1_idx']]
            d2 = keywords[patt['domain2_idx']]
            domain = f"{d1}.{d2}"

            path_parts = [keywords[i] for i in patt['path_indices'] if keywords[i]]
            path_str = patt['path_separator'].join(path_parts)

            fn = keywords[patt['filename_idx']]
            ext = keywords[patt['extension_idx']]

            url = f"{proto}://{domain}/{path_str}/{fn}.{ext}"
            print(f"[í‚¤ì›Œë“œì¬êµ¬ì„±] {p_name} ìƒì„±: {url}")

            if proto.lower() in ('http', 'https') and '.' in domain and ext.lower() == 'm3u8':
                return url
            else:
                print(f"[í‚¤ì›Œë“œì¬êµ¬ì„±] {p_name}: ìœ íš¨í•˜ì§€ ì•Šì€ URL")
        except (IndexError, Exception) as e:
            print(f"[í‚¤ì›Œë“œì¬êµ¬ì„±] {p_name} ì‹¤íŒ¨: {e}")

    return None


def _select_quality(variants, quality):
    """í™”ì§ˆ ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ variantë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
    if not variants:
        return None
    if quality == "worst":
        return variants[-1]
    if quality == "best" or not quality:
        return variants[0]
    # í•´ìƒë„ ê¸°ë°˜ ì„ íƒ (ì˜ˆ: "720p" â†’ ë†’ì´ 720 ì´í•˜ ì¤‘ ìµœê³ )
    m = re.match(r'(\d+)p', quality)
    if m:
        target_h = int(m.group(1))
        for v in variants:
            res = v.get("resolution", "")
            if 'x' in res:
                h = int(res.split('x')[1])
                if h <= target_h:
                    return v
        return variants[-1]
    return variants[0]


def _custom_extract(url: str):
    """hitomi.py ë¡œì§ ê¸°ë°˜ MissAV ì»¤ìŠ¤í…€ ì¶”ì¶œê¸° (Cloudflare ìš°íšŒ)"""
    parsed = urllib.parse.urlparse(url)
    page, session, method = _fetch_page_with_cf_bypass(url)
    soup = BeautifulSoup(page, 'html.parser')

    # ì œëª© ì¶”ì¶œ
    title = "video"
    h1 = soup.find('h1')
    if h1 and h1.text.strip():
        title = h1.text.strip()[:80]
    else:
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            title = og_title['content'].strip()[:80]

    # ì¸ë„¤ì¼ ì¶”ì¶œ
    thumbnail = ""
    og_img = soup.find('meta', {'property': 'og:image'})
    if og_img and og_img.get('content'):
        thumbnail = og_img['content']

    # M3U8 URL ì¶”ì¶œ - P.A.C.K.E.R. ë‚œë…í™” í•´ì œ
    m3u8_url = None

    for script in soup.find_all('script'):
        if not script.string:
            continue
        content = script.string

        # P.A.C.K.E.R. íŒ¨í„´ ê°ì§€
        packer_match = re.search(
            r"eval\s*\(\s*function\s*\(p,\s*a,\s*c,\s*k,\s*e,\s*d\s*\)\s*\{.*?return\s+p}\s*\((.*)\)\)",
            content, re.DOTALL | re.IGNORECASE
        )
        if packer_match:
            print(f"[P.A.C.K.E.R.] ë°œê²¬ë¨, ë””ì½”ë”© ì‹œë„...")
            args_str = packer_match.group(1).strip()

            # pcode ì¶”ì¶œ
            pcode = ""
            pcode_match = re.match(r"(['\"])(.*?)\1\s*,", args_str)
            if pcode_match:
                quote = pcode_match.group(1)
                pcode_raw = pcode_match.group(2)
                pcode = pcode_raw.replace(f"\\{quote}", quote)
            else:
                print("[P.A.C.K.E.R.] pcode ì¶”ì¶œ ì‹¤íŒ¨")
                continue

            # base, count ì¶”ì¶œ
            remaining_after_pcode = args_str[pcode_match.end():]
            nums = re.findall(r'(\d+)', remaining_after_pcode)
            base_n = int(nums[0]) if len(nums) >= 1 else 36
            count = int(nums[1]) if len(nums) >= 2 else 0

            # keyword ë¬¸ìì—´ ì¶”ì¶œ (.split('|') ì•ì˜ ë¬¸ìì—´)
            kstr = ""
            kstr_match = re.search(
                r",\s*(['\"])((?:\\.|(?!\1)[^\\\r\n])*)\1\s*\.split\(\s*['\"]" + re.escape('|') + r"['\"]\s*\)",
                args_str, re.VERBOSE
            )
            if kstr_match:
                kq = kstr_match.group(1)
                kstr_raw = kstr_match.group(2)
                kstr = kstr_raw.replace(f"\\{kq}", kq)

            print(f"[P.A.C.K.E.R.] base={base_n}, count={count}, keywords={len(kstr.split('|')) if kstr else 0}ê°œ")

            # â”€â”€ ë°©ë²• 1: pcodeì—ì„œ ì§ì ‘ M3U8 URL ê²€ìƒ‰ â”€â”€
            direct_m3u8 = re.search(
                r"""(?:file|source|src|f)\s*[:=]\s*(['"])(https?://[^\s"'<>]+\.m3u8[^\s"'<>]*)\1""",
                pcode, re.IGNORECASE
            )
            if direct_m3u8:
                m3u8_url = direct_m3u8.group(2)
                print(f"[P.A.C.K.E.R.] ì§ì ‘ M3U8 ë°œê²¬: {m3u8_url}")
                break

            simple_m3u8 = re.search(r"(https?://[^\s\"'<>]+\.m3u8[^\s\"'<>]*)", pcode)
            if simple_m3u8:
                m3u8_url = simple_m3u8.group(1)
                print(f"[P.A.C.K.E.R.] ë‹¨ìˆœ M3U8 ë°œê²¬: {m3u8_url}")
                break

            # â”€â”€ ë°©ë²• 2: P.A.C.K.E.R. ì‹¤ì œ ì–¸íŒ© (base-N í† í° â†’ í‚¤ì›Œë“œ ì¹˜í™˜) â”€â”€
            if kstr:
                unpacked = _unpack_packer(pcode, base_n, count, kstr)
                if unpacked:
                    print(f"[P.A.C.K.E.R.] ì–¸íŒ© ì™„ë£Œ ({len(unpacked)} chars)")
                    # ì–¸íŒ©ëœ ì½”ë“œì—ì„œ M3U8 ê²€ìƒ‰
                    m3u8_in_unpacked = re.search(
                        r"""(?:file|source|src|f)\s*[:=]\s*(['"])(https?://[^\s"'<>]+\.m3u8[^\s"'<>]*)\1""",
                        unpacked, re.IGNORECASE
                    )
                    if m3u8_in_unpacked:
                        m3u8_url = m3u8_in_unpacked.group(2)
                        print(f"[P.A.C.K.E.R.] ì–¸íŒ© í›„ M3U8 ë°œê²¬: {m3u8_url}")
                        break
                    m3u8_simple_unpacked = re.search(r"(https?://[^\s\"'<>]+\.m3u8[^\s\"'<>]*)", unpacked)
                    if m3u8_simple_unpacked:
                        m3u8_url = m3u8_simple_unpacked.group(1)
                        print(f"[P.A.C.K.E.R.] ì–¸íŒ© í›„ ë‹¨ìˆœ M3U8 ë°œê²¬: {m3u8_url}")
                        break

            # â”€â”€ ë°©ë²• 3: í‚¤ì›Œë“œ ê¸°ë°˜ URL ì¬êµ¬ì„± (hitomi.py ë°©ì‹) â”€â”€
            if kstr:
                reconstructed = _reconstruct_m3u8_from_keywords(kstr)
                if reconstructed:
                    m3u8_url = reconstructed
                    print(f"[P.A.C.K.E.R.] í‚¤ì›Œë“œ ì¬êµ¬ì„± M3U8: {m3u8_url}")
                    break

        # P.A.C.K.E.R.ê°€ ì•„ë‹Œ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì§ì ‘ M3U8 ê²€ìƒ‰ (í´ë°±)
        m3u8_simple = re.search(r"(https?://[^\s\"'<>]+\.m3u8[^\s\"'<>]*)", content)
        if m3u8_simple:
            m3u8_url = m3u8_simple.group(1)
            break

    # í˜ì´ì§€ ì „ì²´ì—ì„œ ë§ˆì§€ë§‰ í´ë°±
    if not m3u8_url:
        m3u8_page = re.search(r"(https?://[^\s\"'<>]+\.m3u8[^\s\"'<>]*)", page)
        if m3u8_page:
            m3u8_url = m3u8_page.group(1)

    if not m3u8_url:
        # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
        cf_signs = ['cf-browser-verification', 'Just a moment', 'Checking your browser',
                    'cf-turnstile', 'challenge-platform', 'Verify you are human']
        is_cf = any(sign.lower() in page.lower() for sign in cf_signs)
        page_title = soup.find('title')
        title_text = page_title.text.strip() if page_title else '(title ì—†ìŒ)'
        print(f"\n{'='*50}")
        print(f"[ì§„ë‹¨] M3U8 ì¶”ì¶œ ì‹¤íŒ¨")
        print(f"  URL: {url}")
        print(f"  ì¶”ì¶œ ë°©ë²•: {method}")
        print(f"  í˜ì´ì§€ í¬ê¸°: {len(page)} bytes")
        print(f"  í˜ì´ì§€ ì œëª©: {title_text}")
        print(f"  Cloudflare ì°¨ë‹¨: {'âœ“ ì°¨ë‹¨ë¨ (ì¿ í‚¤/ë°©ì‹ ë³€ê²½ í•„ìš”)' if is_cf else 'âœ— ì•„ë‹˜'}")
        print(f"  P.A.C.K.E.R. ë°œê²¬: {'eval(function(p,a,c,k' in page}")
        print(f"  ìŠ¤í¬ë¦½íŠ¸ ìˆ˜: {len(soup.find_all('script'))}")
        print(f"  í˜ì´ì§€ ì•ë¶€ë¶„: {page[:500]}")
        print(f"{'='*50}\n")

        if is_cf:
            browser = _get_browser()
            if browser:
                raise ValueError(
                    f"Cloudflareê°€ ì°¨ë‹¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. "
                    f"ê°ì§€ëœ ë¸Œë¼ìš°ì €: {browser}. "
                    f"í•´ë‹¹ ë¸Œë¼ìš°ì €ì—ì„œ ì´ ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ Cloudflare ì²´í¬ë¥¼ ë¨¼ì € í†µê³¼í•´ì£¼ì„¸ìš”. "
                    f"(ì‚¬ì´íŠ¸ê°€ ì—´ë¦¬ë©´ ì´ ì•±ì—ì„œ ë‹¤ì‹œ ì‹œë„)"
                )
            else:
                raise ValueError(
                    "Cloudflareê°€ ì°¨ë‹¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. "
                    "ë¸Œë¼ìš°ì €ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                    "Chrome ë˜ëŠ” Edgeë¥¼ ì„¤ì¹˜í•˜ê³ , í•´ë‹¹ ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ CF ì²´í¬ë¥¼ í†µê³¼í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                )
        raise ValueError("M3U8 URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # M3U8 ë§ˆìŠ¤í„° í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ â†’ ìµœê³  í™”ì§ˆ ì„ íƒ
    referer = f'{parsed.scheme}://{parsed.netloc}/'
    m3u8_headers = {
        'User-Agent': USER_AGENT,
        'Referer': referer,
        'Origin': referer.rstrip('/'),
    }

    all_variants = []
    try:
        m3u8_resp = session.get(m3u8_url, headers=m3u8_headers, timeout=15)
        m3u8_resp.raise_for_status()
        m3u8_content = m3u8_resp.text

        if '#EXT-X-STREAM-INF:' in m3u8_content:
            lines = m3u8_content.strip().split('\n')
            for i, line in enumerate(lines):
                if line.startswith('#EXT-X-STREAM-INF:'):
                    bw = 0
                    res = ""
                    bw_match = re.search(r'BANDWIDTH=(\d+)', line)
                    if bw_match:
                        bw = int(bw_match.group(1))
                    res_match = re.search(r'RESOLUTION=(\d+x\d+)', line)
                    if res_match:
                        res = res_match.group(1)
                    if i + 1 < len(lines) and not lines[i + 1].startswith('#'):
                        variant_url = urllib.parse.urljoin(m3u8_url, lines[i + 1].strip())
                        all_variants.append({"bandwidth": bw, "resolution": res, "url": variant_url})

            if all_variants:
                all_variants.sort(key=lambda x: x["bandwidth"], reverse=True)
                settings = _load_settings()
                quality = settings.get("quality", "best")
                selected = _select_quality(all_variants, quality)
                if selected:
                    m3u8_url = selected["url"]
                    print(f"[í™”ì§ˆ] {quality} â†’ {selected.get('resolution', '?')} ({selected['bandwidth']}bps)")
                else:
                    m3u8_url = all_variants[0]["url"]
    except Exception as e:
        print(f"[M3U8 ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜] {e}")

    # yt-dlp í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return {
        "title": title,
        "url": m3u8_url,
        "thumbnail": thumbnail,
        "duration": 0,
        "http_headers": m3u8_headers,
        "ext": "mp4",
        "_custom_extracted": True,
        "_variants": [{"resolution": v["resolution"], "bandwidth": v["bandwidth"]} for v in all_variants],
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MissAV ê²€ìƒ‰/ê´€ë ¨ ì˜ìƒ ìŠ¤í¬ë˜í•‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _parse_video_cards(soup, base_url):
    """MissAV í˜ì´ì§€ì—ì„œ ì˜ìƒ ì¹´ë“œ ëª©ë¡ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    ê²€ìƒ‰ ê²°ê³¼, ê´€ë ¨ ì˜ìƒ ë“±ì— ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

    ì „ëµ:
    1. ë¹„ë””ì˜¤ ê·¸ë¦¬ë“œ ì»¨í…Œì´ë„ˆ(div.grid)ë¥¼ ì°¾ì•„ì„œ ê·¸ ì•ˆì˜ ì¹´ë“œë§Œ íŒŒì‹± (ìš°ì„ )
    2. ê·¸ë¦¬ë“œë¥¼ ëª» ì°¾ìœ¼ë©´, ì „ì²´ <a> íƒœê·¸ì—ì„œ ì—„ê²©í•œ URL í•„í„°ë§ìœ¼ë¡œ í´ë°±
    """
    results = []
    seen_urls = set()
    parsed_base = urllib.parse.urlparse(base_url)
    base_origin = f"{parsed_base.scheme}://{parsed_base.netloc}"

    # â”€â”€ ë¹„ë””ì˜¤ slug íŒë³„ ë„ìš°ë¯¸ â”€â”€
    def _is_video_slug(slug):
        """ë¹„ë””ì˜¤ ì½”ë“œ íŒ¨í„´ì¸ì§€ í™•ì¸ (ì˜ˆ: abw-366, fc2-ppv-1234567, ssis-001)"""
        # ë°˜ë“œì‹œ í•˜ì´í”ˆ ë’¤ì— ìˆ«ìê°€ ìˆì–´ì•¼ í•¨
        if not re.search(r'-\d', slug):
            return False
        # ë„ˆë¬´ ì§§ì€ ìŠ¬ëŸ¬ê·¸ ì œì™¸
        if len(slug) < 4:
            return False
        # ì˜ë¬¸+ìˆ«ì+í•˜ì´í”ˆ+ì–¸ë”ìŠ¤ì½”ì–´ë§Œ í—ˆìš©
        if not re.match(r'^[a-zA-Z0-9][-_a-zA-Z0-9]*$', slug):
            return False
        return True

    def _extract_card_data(a_tag, container=None):
        """<a> íƒœê·¸ì™€ ì»¨í…Œì´ë„ˆì—ì„œ ì¹´ë“œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        href = a_tag.get('href', '')
        full_url = urllib.parse.urljoin(base_origin + '/', href)

        # ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë©´ ê±´ë„ˆëœ€
        if full_url in seen_urls:
            return None
        seen_urls.add(full_url)

        # URL ê²€ì¦
        up = urllib.parse.urlparse(full_url)
        if not any(d in up.netloc for d in CUSTOM_DOMAINS):
            return None
        path_parts = [p for p in up.path.strip('/').split('/') if p]
        if not path_parts:
            return None
        slug = path_parts[-1]
        if not _is_video_slug(slug):
            return None

        # íƒìƒ‰ ë²”ìœ„ ê²°ì • (ì»¨í…Œì´ë„ˆ â†’ a íƒœê·¸ì˜ ë¶€ëª¨ â†’ a íƒœê·¸ ìì²´)
        card = container or a_tag

        # ì¸ë„¤ì¼ ì¶”ì¶œ
        thumb = ''
        img = card.find('img')
        if img:
            thumb = img.get('data-src') or img.get('src') or img.get('data-original') or ''
            # 1x1 placeholder ì´ë¯¸ì§€ ë¬´ì‹œ
            if thumb and ('base64' in thumb or len(thumb) < 20):
                thumb = img.get('data-src') or ''
            if thumb and not thumb.startswith('http'):
                thumb = urllib.parse.urljoin(base_origin + '/', thumb)

        # ì œëª© ì¶”ì¶œ: img alt â†’ a alt â†’ a title â†’ í…ìŠ¤íŠ¸
        title = ''
        if img:
            title = (img.get('alt') or '').strip()
        if not title:
            title = (a_tag.get('alt') or a_tag.get('title', '')).strip()
        if not title:
            # ì¹´ë“œ ë‚´ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
            for tag_name in ['h3', 'h2', 'h4', 'span', 'div']:
                text_el = card.find(tag_name, string=True)
                if text_el and len(text_el.text.strip()) > 3:
                    title = text_el.text.strip()
                    break
        if not title:
            title = slug

        # Duration ì¶”ì¶œ: ì¹´ë“œ/ì»¨í…Œì´ë„ˆ ë‚´ ì‹œê°„ íŒ¨í„´
        duration = ''
        dur_pattern = re.compile(r'\b(\d{1,3}:\d{2}(?::\d{2})?)\b')
        search_area = container or card
        for text_node in search_area.find_all(string=dur_pattern):
            m = dur_pattern.search(str(text_node))
            if m:
                duration = m.group(1)
                break

        return {
            'url': full_url,
            'title': title[:120],
            'thumbnail': thumb,
            'duration': duration,
        }

    # â”€â”€ ì „ëµ 1: ë¹„ë””ì˜¤ ê·¸ë¦¬ë“œ ì»¨í…Œì´ë„ˆì—ì„œ ì¹´ë“œ íŒŒì‹± â”€â”€
    grid = soup.find('div', class_=re.compile(r'grid.*grid-cols'))
    if grid:
        # ê·¸ë¦¬ë“œ ë‚´ ì§ì ‘ ìì‹ì´ ê°ê°ì˜ ì¹´ë“œ
        for card_div in grid.find_all('div', recursive=False):
            # ì¹´ë“œ ë‚´ ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ ë§í¬ ì°¾ê¸°
            for a_tag in card_div.find_all('a', href=True):
                href = a_tag['href']
                if '?' in href:
                    continue
                data = _extract_card_data(a_tag, container=card_div)
                if data:
                    results.append(data)
                    break  # ì¹´ë“œë‹¹ í•˜ë‚˜ë§Œ

    # â”€â”€ ì „ëµ 2: ê·¸ë¦¬ë“œë¥¼ ëª» ì°¾ìœ¼ë©´ ì „ì²´ <a> íƒœê·¸ ìŠ¤ìº” (ì—„ê²© í•„í„°) â”€â”€
    if not results:
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ìˆëŠ” URL ì œì™¸ (ê²€ìƒ‰/í•„í„°/í˜ì´ì§€ë„¤ì´ì…˜)
            if '?' in href:
                continue
            # /search/, /site/ ê²½ë¡œ ì œì™¸
            if '/search/' in href or '/site/' in href:
                continue

            data = _extract_card_data(a_tag)
            if data:
                results.append(data)

    return results


def _extract_related_videos(url: str):
    """ë¹„ë””ì˜¤ í˜ì´ì§€ì—ì„œ ê´€ë ¨(ì¶”ì²œ) ì˜ìƒ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        html, session, method = _fetch_page_with_cf_bypass(url)
        soup = BeautifulSoup(html, 'html.parser')
        parsed = urllib.parse.urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"

        # í˜„ì¬ ì˜ìƒ URL ìì²´ëŠ” ì œì™¸
        all_cards = _parse_video_cards(soup, base_url)
        # í˜„ì¬ í˜ì´ì§€ URLê³¼ ë™ì¼í•œ í•­ëª© ì œì™¸
        uid = _url_id(url)
        related = [c for c in all_cards if _url_id(c['url']) != uid]

        print(f"[ê´€ë ¨ ì˜ìƒ] {url[:60]}... â†’ {len(related)}ê°œ ë°œê²¬")
        return related
    except Exception as e:
        print(f"[ê´€ë ¨ ì˜ìƒ] ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¼ìš°íŠ¸ - í˜ì´ì§€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/")
def index():
    import time as _t
    return render_template("index.html", cache_bust=str(int(_t.time())))

@app.route("/search")
def search_page():
    """ê²€ìƒ‰ ì „ìš© í˜ì´ì§€ (ë³„ë„ ì°½)"""
    return render_template("search.html")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - ê²€ìƒ‰ & ê´€ë ¨ ì˜ìƒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/search")
def api_search():
    """MissAVì—ì„œ í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰"""
    q = request.args.get('q', '').strip()
    page = request.args.get('page', 1, type=int)
    sort = request.args.get('sort', '')  # '', 'views', 'published_at', 'likes'
    if not q:
        return jsonify({"results": [], "query": "", "page": 1, "has_next": False})

    # ê²€ìƒ‰ URL êµ¬ì„± (MissAV ê²€ìƒ‰ íŒ¨í„´)
    base = 'https://missav.ws'
    encoded_q = urllib.parse.quote(q)
    search_url = f"{base}/search/{encoded_q}"
    params = []
    if page > 1:
        params.append(f"page={page}")
    if sort:
        params.append(f"sort={sort}")
    if params:
        search_url += '?' + '&'.join(params)

    try:
        html, session, method = _fetch_page_with_cf_bypass(search_url)
        soup = BeautifulSoup(html, 'html.parser')
        results = _parse_video_cards(soup, base)

        # ë‹¤ìŒ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        has_next = False
        # ì¼ë°˜ì ì¸ í˜ì´ì§€ë„¤ì´ì…˜: ë‹¤ìŒ/next ë§í¬ ë˜ëŠ” í˜„ì¬ í˜ì´ì§€+1 ë§í¬
        for a in soup.find_all('a', href=True):
            if f'page={page + 1}' in a['href']:
                has_next = True
                break
            # rel="next" íŒ¨í„´
            if a.get('rel') and 'next' in a.get('rel', []):
                has_next = True
                break

        print(f"[ê²€ìƒ‰] '{q}' (page={page}) â†’ {len(results)}ê°œ ê²°ê³¼, ë‹¤ìŒí˜ì´ì§€={has_next}")
        return jsonify({
            "results": results,
            "query": q,
            "page": page,
            "has_next": has_next,
        })
    except Exception as e:
        print(f"[ê²€ìƒ‰] ì˜¤ë¥˜: {e}")
        return jsonify({"error": str(e), "results": [], "query": q, "page": page, "has_next": False}), 500


@app.route("/api/related")
def api_related():
    """ë¹„ë””ì˜¤ í˜ì´ì§€ì—ì„œ ê´€ë ¨(ì¶”ì²œ) ì˜ìƒ ëª©ë¡ ë°˜í™˜"""
    url = request.args.get('url', '').strip()
    if not url:
        return jsonify({"related": [], "error": "URL í•„ìˆ˜"})
    related = _extract_related_videos(url)
    return jsonify({"related": related})


@app.route("/api/open-tab", methods=["POST"])
def open_tab():
    """ì§€ì • URLì„ ìƒˆ pywebview íƒ­(ì‚¬ì´íŠ¸ ë¸Œë¼ìš°ì € ì°½)ìœ¼ë¡œ ì—½ë‹ˆë‹¤.
    ëŒ€ê¸°ì—´ì˜ ì˜ìƒ URLì„ ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ì¬ìƒí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    body = request.json
    url = body.get("url", "").strip()
    if not url:
        return jsonify({"ok": False, "error": "URLì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400

    if not _webview_ready.is_set():
        return jsonify({"ok": False, "error": "pywebview not ready"})

    try:
        _open_browse_tab(url)
        return jsonify({"ok": True})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)})


@app.route("/api/open-search", methods=["POST"])
def open_search_window():
    """MissAV ì‚¬ì´íŠ¸ë¥¼ pywebview ì°½ì—ì„œ ì§ì ‘ ì—´ê¸°.
    ì‚¬ì´íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ íƒìƒ‰í•˜ë©´ì„œ ëŒ€ê¸°ì—´ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    JS ì¸ì ì…˜ìœ¼ë¡œ ê° ì˜ìƒ ì¹´ë“œì— [+] ë²„íŠ¼ì„ ì˜¤ë²„ë ˆì´í•©ë‹ˆë‹¤."""
    global _search_window

    if not _webview_ready.is_set():
        return jsonify({"ok": False, "error": "pywebview not ready"})

    try:
        import webview
    except ImportError:
        return jsonify({"ok": False, "error": "pywebview not installed"})

    def _inject_js_safe(win_ref, label="íƒìƒ‰ì°½"):
        """JS ì¸ì ì…˜ì„ ì•ˆì „í•˜ê²Œ ìˆ˜í–‰ (ì§€ì—° + ì¬ì‹œë„)"""
        import time as _time
        _time.sleep(1.5)
        for attempt in range(5):
            try:
                win_ref.evaluate_js(_BROWSE_INJECT_JS)
                print(f"  [{label}] JS ì¸ì ì…˜ ì„±ê³µ (ì‹œë„ {attempt+1})")
                return True
            except Exception as e:
                print(f"  [{label}] JS ì¸ì ì…˜ ì‹œë„ {attempt+1}/5 ì‹¤íŒ¨: {e}")
                if attempt < 4:
                    _time.sleep(1.0)
        print(f"  [{label}] JS ì¸ì ì…˜ ìµœì¢… ì‹¤íŒ¨")
        return False

    try:
        # ì´ë¯¸ ìƒì„±ëœ ì°½ì´ ìˆìœ¼ë©´ show() + JS ì¬ì¸ì ì…˜
        if _search_window is not None and _search_window in webview.windows:
            _search_window.show()
            # show() í›„ì—ë„ JS ì¬ì¸ì ì…˜ (í˜ì´ì§€ ì´ë™ í›„ ì‚¬ë¼ì¡Œì„ ìˆ˜ ìˆìŒ)
            threading.Thread(
                target=_inject_js_safe,
                args=(_search_window, "íƒìƒ‰ì°½-show"),
                daemon=True
            ).start()
            return jsonify({"ok": True, "action": "shown"})

        # ì‚¬ì´íŠ¸ ë¸Œë¼ìš°ì € ì°½ ë™ì  ìƒì„±
        _search_window = webview.create_window(
            title="MissAV â€” StreamPlayer íƒìƒ‰",
            url="https://missav.ws",
            width=1100,
            height=800,
            min_size=(700, 500),
            text_select=True,
            js_api=_browse_api,
        )

        if _search_window is None:
            return jsonify({"ok": False, "error": "Window creation returned None"})

        # í´ë¡œì € ì°¸ì¡° ì•ˆì „í•˜ê²Œ ìº¡ì²˜
        _sw_ref = _search_window

        # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ì‹œ JS ì¸ì ì…˜ (í˜ì´ì§€ ì´ë™ë§ˆë‹¤ ì¬ì¸ì ì…˜)
        def _on_browse_loaded():
            threading.Thread(
                target=_inject_js_safe,
                args=(_sw_ref, "íƒìƒ‰ì°½-loaded"),
                daemon=True
            ).start()

        _search_window.events.loaded += _on_browse_loaded

        # â˜… í•µì‹¬ ìˆ˜ì •: ìµœì´ˆ ìƒì„± ì‹œ loaded ì´ë²¤íŠ¸ ëˆ„ë½ ëŒ€ë¹„ ìˆ˜ë™ ì¸ì ì…˜
        # create_window() í›„ í˜ì´ì§€ê°€ ì´ë¯¸ ë¡œë“œë˜ì–´ loaded ì´ë²¤íŠ¸ë¥¼ ë†“ì¹  ìˆ˜ ìˆìŒ
        threading.Thread(
            target=_inject_js_safe,
            args=(_sw_ref, "íƒìƒ‰ì°½-ì´ˆê¸°"),
            daemon=True
        ).start()

        # X ë²„íŠ¼ â†’ íŒŒê´´í•˜ì§€ ì•Šê³  ìˆ¨ê¸°ê¸°ë§Œ
        def _on_search_closing():
            try:
                _search_window.hide()
            except Exception:
                pass
            return False   # ì‹¤ì œ ë‹«ê¸° ì·¨ì†Œ
        _search_window.events.closing += _on_search_closing

        print("  [íƒìƒ‰ì°½] MissAV ì‚¬ì´íŠ¸ ë¸Œë¼ìš°ì € ìƒì„± ì™„ë£Œ")
        return jsonify({"ok": True, "action": "created"})

    except Exception as e:
        print(f"[íƒìƒ‰ì°½] ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"ok": False, "error": str(e)})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - ëŒ€ê¸°ì—´ ê´€ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/queue", methods=["GET"])
def get_queue():
    data = _load_data()
    return jsonify(data["queue"])

@app.route("/api/queue", methods=["POST"])
def add_to_queue():
    """URLì„ ëŒ€ê¸°ì—´ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    body = request.json
    url = body.get("url", "").strip()
    if not url:
        return jsonify({"error": "URLì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400

    try:
        info = _extract_info(url)
    except Exception as e:
        return jsonify({"error": f"ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"}), 400

    uid = _url_id(url)
    entry = {
        "id": uid,
        "url": url,
        "title": info.get("title", url),
        "duration": info.get("duration", 0),
        "thumbnail": info.get("thumbnail", ""),
        "added_at": time.time(),
        "stream_url": info.get("url", ""),
        "http_headers": info.get("http_headers", {}),
        "variants": info.get("_variants", []),
    }

    data = _load_data()
    # ì¤‘ë³µ ë°©ì§€
    if any(item["id"] == uid for item in data["queue"]):
        return jsonify({"error": "ì´ë¯¸ ëŒ€ê¸°ì—´ì— ìˆìŠµë‹ˆë‹¤.", "duplicate": True, "title": entry["title"]}), 409
    data["queue"].append(entry)
    _save_data(data)

    return jsonify(entry)

@app.route("/api/queue/<item_id>", methods=["DELETE"])
def delete_from_queue(item_id):
    data = _load_data()
    data["queue"] = [item for item in data["queue"] if item["id"] != item_id]
    # ê´€ë ¨ ì¬ìƒ ìœ„ì¹˜, íˆíŠ¸ë§µë„ ì‚­ì œ
    data["playback"].pop(item_id, None)
    data["heatmaps"].pop(item_id, None)
    _save_data(data)
    return jsonify({"ok": True})

@app.route("/api/queue/clear", methods=["POST"])
def clear_queue():
    data = _load_data()
    data["queue"] = []
    data["playback"] = {}
    data["heatmaps"] = {}
    _save_data(data)
    return jsonify({"ok": True})

@app.route("/api/queue/reorder", methods=["POST"])
def reorder_queue():
    """ëŒ€ê¸°ì—´ ìˆœì„œë¥¼ ë³€ê²½í•©ë‹ˆë‹¤."""
    body = request.json
    id_order = body.get("ids", [])
    if not id_order:
        return jsonify({"error": "ids í•„ìˆ˜"}), 400
    data = _load_data()
    id_map = {item["id"]: item for item in data["queue"]}
    new_queue = []
    for uid in id_order:
        if uid in id_map:
            new_queue.append(id_map[uid])
    # id_orderì— ì—†ëŠ” ê¸°ì¡´ í•­ëª©ë„ ìœ ì§€
    for item in data["queue"]:
        if item["id"] not in id_order:
            new_queue.append(item)
    data["queue"] = new_queue
    _save_data(data)
    return jsonify({"ok": True})

@app.route("/api/queue/move", methods=["POST"])
def move_queue_items():
    """ëŒ€ê¸°ì—´ í•­ëª©ì„ ë§¨ ìœ„ ë˜ëŠ” ë§¨ ì•„ë˜ë¡œ ì´ë™í•©ë‹ˆë‹¤."""
    body = request.json
    item_ids = body.get("ids", [])
    position = body.get("position", "top")  # "top" or "bottom"
    if not item_ids:
        return jsonify({"error": "ids í•„ìˆ˜"}), 400
    data = _load_data()
    id_set = set(item_ids)
    moved = [item for item in data["queue"] if item["id"] in id_set]
    rest = [item for item in data["queue"] if item["id"] not in id_set]
    if position == "top":
        data["queue"] = moved + rest
    else:
        data["queue"] = rest + moved
    _save_data(data)
    return jsonify({"ok": True})

@app.route("/api/queue/bulk-delete", methods=["POST"])
def bulk_delete_queue():
    """ì—¬ëŸ¬ ëŒ€ê¸°ì—´ í•­ëª©ì„ ì¼ê´„ ì‚­ì œí•©ë‹ˆë‹¤."""
    body = request.json
    item_ids = body.get("ids", [])
    if not item_ids:
        return jsonify({"error": "ids í•„ìˆ˜"}), 400
    data = _load_data()
    id_set = set(item_ids)
    data["queue"] = [item for item in data["queue"] if item["id"] not in id_set]
    for uid in item_ids:
        data["playback"].pop(uid, None)
        data["heatmaps"].pop(uid, None)
    _save_data(data)
    return jsonify({"ok": True})

@app.route("/api/queue/bulk-category", methods=["POST"])
def bulk_set_category():
    """ì—¬ëŸ¬ ëŒ€ê¸°ì—´ í•­ëª©ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì¼ê´„ ë³€ê²½í•©ë‹ˆë‹¤."""
    body = request.json
    item_ids = body.get("ids", [])
    category = body.get("category", None)
    if not item_ids:
        return jsonify({"error": "ids í•„ìˆ˜"}), 400
    data = _load_data()
    id_set = set(item_ids)
    for item in data["queue"]:
        if item["id"] in id_set:
            if category:
                item["category"] = category
            else:
                item.pop("category", None)
    _save_data(data)
    return jsonify({"ok": True})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - ì¹´í…Œê³ ë¦¬ ê´€ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/categories", methods=["GET"])
def get_categories():
    """ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ"""
    data = _load_data()
    return jsonify(data.get("categories", []))

@app.route("/api/categories", methods=["POST"])
def create_category():
    """ìƒˆ ì¹´í…Œê³ ë¦¬ ìƒì„±"""
    body = request.json
    name = body.get("name", "").strip()
    if not name:
        return jsonify({"error": "ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”."}), 400
    color = body.get("color", "#4a9eff")
    cat_id = "cat_" + hashlib.md5(f"{name}{time.time()}".encode()).hexdigest()[:8]
    cat = {"id": cat_id, "name": name, "color": color}
    data = _load_data()
    data.setdefault("categories", []).append(cat)
    _save_data(data)
    return jsonify(cat)

@app.route("/api/categories/<cat_id>", methods=["PUT"])
def update_category(cat_id):
    """ì¹´í…Œê³ ë¦¬ ìˆ˜ì • (ì´ë¦„/ìƒ‰ìƒ)"""
    body = request.json
    data = _load_data()
    for cat in data.get("categories", []):
        if cat["id"] == cat_id:
            if "name" in body:
                cat["name"] = body["name"].strip()
            if "color" in body:
                cat["color"] = body["color"]
            _save_data(data)
            return jsonify(cat)
    return jsonify({"error": "ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404

@app.route("/api/categories/<cat_id>", methods=["DELETE"])
def delete_category(cat_id):
    """ì¹´í…Œê³ ë¦¬ ì‚­ì œ (í•­ëª©ì€ ë¯¸ë¶„ë¥˜ë¡œ)"""
    data = _load_data()
    data["categories"] = [c for c in data.get("categories", []) if c["id"] != cat_id]
    # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ í•­ëª©ë“¤ì„ ë¯¸ë¶„ë¥˜ë¡œ
    for item in data["queue"]:
        if item.get("category") == cat_id:
            item.pop("category", None)
    _save_data(data)
    return jsonify({"ok": True})

@app.route("/api/categories/reorder", methods=["POST"])
def reorder_categories():
    """ì¹´í…Œê³ ë¦¬ ìˆœì„œ ë³€ê²½"""
    body = request.json
    id_order = body.get("ids", [])
    if not id_order:
        return jsonify({"error": "ids í•„ìˆ˜"}), 400
    data = _load_data()
    id_map = {c["id"]: c for c in data.get("categories", [])}
    new_cats = [id_map[cid] for cid in id_order if cid in id_map]
    for c in data.get("categories", []):
        if c["id"] not in id_order:
            new_cats.append(c)
    data["categories"] = new_cats
    _save_data(data)
    return jsonify({"ok": True})

@app.route("/api/queue/<item_id>/category", methods=["POST"])
def set_item_category(item_id):
    """ëŒ€ê¸°ì—´ í•­ëª©ì˜ ì¹´í…Œê³ ë¦¬ ì„¤ì •"""
    body = request.json
    category = body.get("category")  # Noneì´ë©´ ë¯¸ë¶„ë¥˜
    data = _load_data()
    for item in data["queue"]:
        if item["id"] == item_id:
            if category:
                item["category"] = category
            else:
                item.pop("category", None)
            _save_data(data)
            return jsonify({"ok": True, "category": category})
    return jsonify({"error": "í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - ì¬ìƒ ìœ„ì¹˜ ê¸°ì–µ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/playback/<item_id>", methods=["GET"])
def get_playback(item_id):
    data = _load_data()
    pb = data.get("playback", {}).get(item_id, {"position": 0})
    return jsonify(pb)

@app.route("/api/playback/<item_id>", methods=["POST"])
def save_playback(item_id):
    body = request.json
    data = _load_data()
    if "playback" not in data:
        data["playback"] = {}
    data["playback"][item_id] = {
        "position": body.get("position", 0),
        "updated_at": time.time(),
    }
    _save_data(data)
    return jsonify({"ok": True})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - íˆíŠ¸ë§µ (ìì£¼ ë°˜ë³µ êµ¬ê°„)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/heatmap/<item_id>", methods=["GET"])
def get_heatmap(item_id):
    data = _load_data()
    hm = data.get("heatmaps", {}).get(item_id, {})
    return jsonify(hm)

@app.route("/api/heatmap/<item_id>", methods=["POST"])
def save_heatmap(item_id):
    """ì¬ìƒ ì¤‘ í˜„ì¬ ìœ„ì¹˜(ì´ˆ ë‹¨ìœ„)ë¥¼ ê¸°ë¡í•˜ì—¬ íˆíŠ¸ë§µì„ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    body = request.json
    second = int(body.get("second", 0))
    data = _load_data()
    if "heatmaps" not in data:
        data["heatmaps"] = {}
    if item_id not in data["heatmaps"]:
        data["heatmaps"][item_id] = {}
    key = str(second)
    data["heatmaps"][item_id][key] = data["heatmaps"][item_id].get(key, 0) + 1
    _save_data(data)
    return jsonify({"ok": True})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - ì˜ìƒ ìŠ¤íŠ¸ë¦¼ í”„ë¡ì‹œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/stream")
def stream_video():
    """yt-dlpë¡œ ì¶”ì¶œí•œ ì§ì ‘ URLì„ í”„ë¡ì‹œí•˜ì—¬ ë¸Œë¼ìš°ì €ì— ì „ë‹¬í•©ë‹ˆë‹¤."""
    url = request.args.get("url", "")
    if not url:
        return "URL required", 400

    try:
        # ëŒ€ê¸°ì—´ì— ì €ì¥ëœ stream_urlì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ì‚¬ìš© (ì¬ì¶”ì¶œ ë¶ˆí•„ìš”)
        uid = _url_id(url)
        data = _load_data()
        queue_item = next((q for q in data["queue"] if q["id"] == uid), None)
        stored_stream_url = queue_item.get("stream_url", "") if queue_item else ""
        stored_headers = queue_item.get("http_headers", {}) if queue_item else {}

        if stored_stream_url:
            video_url = stored_stream_url
            http_headers = stored_headers
            print(f"[ìŠ¤íŠ¸ë¦¼] ì €ì¥ëœ URL ì‚¬ìš© (ì¦‰ì‹œ): {video_url[:80]}...")
        else:
            info = _extract_info(url)
            video_url = info.get("url")
            if not video_url:
                formats = info.get("formats", [])
                if formats:
                    best = formats[-1]
                    video_url = best.get("url")
            if not video_url:
                return "ìŠ¤íŠ¸ë¦¼ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", 404
            http_headers = info.get("http_headers", {})
            # ëŒ€ê¸°ì—´ì— stream_url ì €ì¥ (ë‹¤ìŒë²ˆ ì¦‰ì‹œ ì‚¬ìš©)
            if queue_item and video_url:
                queue_item["stream_url"] = video_url
                queue_item["http_headers"] = http_headers
                _save_data(data)
    except Exception as e:
        return f"ì¶”ì¶œ ì˜¤ë¥˜: {e}", 500

    # í—¤ë” êµ¬ì„± (Referer, Cookie ë“±)
    headers = {'User-Agent': USER_AGENT}
    if http_headers:
        headers.update(http_headers)

    # HLS(m3u8) ìŠ¤íŠ¸ë¦¼ì¸ ê²½ìš°: ìºì‹œëœ M3U8 ì¦‰ì‹œ ë°˜í™˜, ì—†ìœ¼ë©´ ê°€ì ¸ì™€ì„œ ìºì‹œ
    if '.m3u8' in video_url:
        try:
            content = _fetch_and_cache_m3u8(video_url, headers)
            response_headers = {
                'Access-Control-Allow-Origin': '*',
                'Content-Type': 'application/vnd.apple.mpegurl',
                'Cache-Control': 'max-age=300',
            }
            return Response(content, headers=response_headers)
        except Exception as e:
            # URL ë§Œë£Œ ë“±ìœ¼ë¡œ ì‹¤íŒ¨ â†’ ì¬ì¶”ì¶œ ì‹œë„
            if stored_stream_url:
                print(f"[ìŠ¤íŠ¸ë¦¼] M3U8 ë¡œë“œ ì‹¤íŒ¨ ({e}), ì¬ì¶”ì¶œ...")
                try:
                    info = _extract_info(url, use_cache=False)
                    new_url = info.get("url", "")
                    if new_url:
                        if queue_item:
                            queue_item["stream_url"] = new_url
                            queue_item["http_headers"] = info.get("http_headers", {})
                            _save_data(data)
                        new_headers = {'User-Agent': USER_AGENT}
                        new_headers.update(info.get("http_headers", {}))
                        content = _fetch_and_cache_m3u8(new_url, new_headers)
                        response_headers = {
                            'Access-Control-Allow-Origin': '*',
                            'Content-Type': 'application/vnd.apple.mpegurl',
                            'Cache-Control': 'max-age=300',
                        }
                        return Response(content, headers=response_headers)
                except Exception as e2:
                    return f"ì¬ì¶”ì¶œ ì‹¤íŒ¨: {e2}", 500
            return f"M3U8 í”„ë¡ì‹œ ì˜¤ë¥˜: {e}", 500

    # Range ìš”ì²­ ì§€ì›
    range_header = request.headers.get("Range")
    if range_header:
        headers["Range"] = range_header

    try:
        resp = requests.get(video_url, headers=headers, stream=True, timeout=30)
        excluded = {"content-encoding", "transfer-encoding", "connection"}
        response_headers = {
            k: v for k, v in resp.headers.items() if k.lower() not in excluded
        }
        response_headers["Access-Control-Allow-Origin"] = "*"

        return Response(
            stream_with_context(resp.iter_content(chunk_size=1024 * 64)),
            status=resp.status_code,
            headers=response_headers,
            content_type=resp.headers.get("Content-Type", "video/mp4"),
        )
    except Exception as e:
        return f"ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {e}", 500

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - ì˜ìƒ ë‹¤ìš´ë¡œë“œ (ëŒ€ê¸°ì—´ ì‹œìŠ¤í…œ, ìµœëŒ€ 2ê°œ ë™ì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_download_status = {}  # id -> {status, progress, filename, error, title, url}
_download_queue = []   # [{uid, url, title}, ...] - ëŒ€ê¸° ì¤‘ì¸ ë‹¤ìš´ë¡œë“œ
_download_active = 0   # í˜„ì¬ ë‹¤ìš´ë¡œë“œ ì¤‘ì¸ ìˆ˜
_download_lock = threading.Lock()
_MAX_CONCURRENT_DL = 1  # 1ê°œì”© ìˆœì°¨ ë‹¤ìš´ë¡œë“œ (ì•ˆì •ì„± + ì†ë„ ìš°ì„ )

def _sanitize_filename(name: str) -> str:
    """íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°"""
    name = re.sub(r'[\\/:*?"<>|]', '_', name)
    name = name.strip('. ')
    return name[:200] if name else 'video'

def _process_download_queue():
    """ëŒ€ê¸°ì—´ì—ì„œ ë‹¤ìŒ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
    global _download_active
    with _download_lock:
        while _download_active < _MAX_CONCURRENT_DL and _download_queue:
            item = _download_queue.pop(0)
            _download_active += 1
            t = threading.Thread(target=_do_download_worker, args=(item,), daemon=True)
            t.start()

def _do_download_worker(dl_item):
    """1ê°œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰ (ìˆœì°¨, ë™ê¸°)"""
    global _download_active
    uid = dl_item["uid"]
    url = dl_item["url"]
    title = dl_item["title"]
    out_filepath = None
    try:
        with _download_lock:
            _download_status[uid]["status"] = "downloading"
        print(f"[ë‹¤ìš´ë¡œë“œ] ì‹œì‘: {title[:60]}")

        # ë‹¤ìš´ë¡œë“œ í´ë” ì„¤ì •
        settings = _load_settings()
        dl_folder = settings.get("downloadFolder", "").strip()
        if dl_folder and os.path.isdir(dl_folder):
            out_dir = Path(dl_folder)
        else:
            out_dir = DOWNLOADS_DIR
        out_dir.mkdir(exist_ok=True)

        # ëŒ€ê¸°ì—´ì—ì„œ ì €ì¥ëœ ìŠ¤íŠ¸ë¦¼ URL í™•ì¸
        data = _load_data()
        queue_item = next((q for q in data["queue"] if q["id"] == uid), None)
        stream_url = queue_item.get("stream_url", "") if queue_item else ""
        stored_headers = queue_item.get("http_headers", {}) if queue_item else {}

        # stream_urlì´ ì—†ìœ¼ë©´ ì»¤ìŠ¤í…€ ë„ë©”ì¸ì¸ ê²½ìš° ì¬ì¶”ì¶œ
        parsed = urllib.parse.urlparse(url)
        is_custom = any(d in parsed.netloc for d in CUSTOM_DOMAINS)
        if not stream_url and is_custom:
            print(f"  [ë‹¤ìš´ë¡œë“œ] stream_url ì—†ìŒ â†’ ì»¤ìŠ¤í…€ ì¶”ì¶œê¸°ë¡œ ì¬ì¶”ì¶œ")
            try:
                re_info = _custom_extract(url)
                stream_url = re_info.get("url", "")
                stored_headers = re_info.get("http_headers", {})
                if queue_item and stream_url:
                    queue_item["stream_url"] = stream_url
                    queue_item["http_headers"] = stored_headers
                    _save_data(data)
            except Exception as e:
                print(f"  [ë‹¤ìš´ë¡œë“œ] ì¬ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # íŒŒì¼ëª… ì„¤ì •
        safe_title = _sanitize_filename(title)
        out_template = str(out_dir / f"{safe_title}.%(ext)s")

        download_url = url
        if stream_url:
            download_url = stream_url
            opts = {
                "quiet": True,
                "no_warnings": True,
                "format": "best",
                "noplaylist": True,
                "skip_download": False,
                "outtmpl": out_template,
                # â”€â”€ ì†ë„ ìµœì í™” â”€â”€
                "concurrent_fragment_downloads": 4,      # 4ê°œ í”„ë˜ê·¸ë¨¼íŠ¸ ë™ì‹œ ë‹¤ìš´ë¡œë“œ
                "buffersize": 1024 * 256,                # 256KB ë²„í¼
                "http_chunk_size": 1024 * 1024 * 50,     # 50MB ì²­í¬
                "retries": 10,
                "fragment_retries": 10,
                "file_access_retries": 5,
                "extractor_retries": 3,
                "noprogress": True,
            }
            if stored_headers:
                opts["http_headers"] = stored_headers
            print(f"  [ë‹¤ìš´ë¡œë“œ] {safe_title} - ì €ì¥ëœ ìŠ¤íŠ¸ë¦¼ URL ì‚¬ìš© (í”„ë˜ê·¸ë¨¼íŠ¸ x4)")
        else:
            opts = _ydl_opts(extract_only=False)
            opts["skip_download"] = False
            opts["outtmpl"] = out_template
            opts["concurrent_fragment_downloads"] = 4
            opts["buffersize"] = 1024 * 256
            opts["retries"] = 10
            opts["fragment_retries"] = 10

        def progress_hook(d):
            nonlocal out_filepath
            if d["status"] == "downloading":
                total = d.get("total_bytes") or d.get("total_bytes_estimate") or 0
                downloaded = d.get("downloaded_bytes", 0)
                speed = d.get("speed") or 0
                with _download_lock:
                    if total > 0:
                        _download_status[uid]["progress"] = round(downloaded / total * 100, 1)
                    if speed > 0:
                        _download_status[uid]["speed"] = speed
            elif d["status"] == "finished":
                out_filepath = d.get("filename", "")
                with _download_lock:
                    _download_status[uid]["progress"] = 100
                    _download_status[uid]["filename"] = out_filepath

        opts["progress_hooks"] = [progress_hook]

        with yt_dlp.YoutubeDL(opts) as ydl:
            ydl.download([download_url])

        # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ .part ì„ì‹œ íŒŒì¼ ì •ë¦¬
        _cleanup_temp_files(out_dir, safe_title)

        with _download_lock:
            _download_status[uid]["status"] = "done"
        print(f"  [ë‹¤ìš´ë¡œë“œ] ì™„ë£Œ: {title[:60]}")

    except Exception as e:
        print(f"  [ë‹¤ìš´ë¡œë“œ] ì˜¤ë¥˜: {title[:40]} â€” {e}")
        with _download_lock:
            _download_status[uid]["status"] = "error"
            _download_status[uid]["error"] = str(e)
    finally:
        with _download_lock:
            _download_active -= 1
        # ëŒ€ê¸°ì—´ ë‹¤ìŒ ì²˜ë¦¬
        _process_download_queue()


def _cleanup_temp_files(out_dir, safe_title):
    """ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ .part, .ytdl ë“± ì„ì‹œ íŒŒì¼ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    import glob
    patterns = [
        str(out_dir / f"{safe_title}*.part"),
        str(out_dir / f"{safe_title}*.part-Frag*"),
        str(out_dir / f"{safe_title}*.ytdl"),
        str(out_dir / f"{safe_title}*.temp"),
    ]
    for pattern in patterns:
        for f in glob.glob(pattern):
            try:
                os.remove(f)
                print(f"  [ì •ë¦¬] ì„ì‹œíŒŒì¼ ì‚­ì œ: {os.path.basename(f)}")
            except OSError:
                pass

@app.route("/api/download", methods=["POST"])
def start_download():
    """ì˜ìƒ ë‹¤ìš´ë¡œë“œë¥¼ ëŒ€ê¸°ì—´ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    body = request.json
    url = body.get("url", "").strip()
    if not url:
        return jsonify({"error": "URLì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400

    uid = _url_id(url)
    if uid in _download_status and _download_status[uid].get("status") in ("downloading", "queued"):
        return jsonify({"error": "ì´ë¯¸ ë‹¤ìš´ë¡œë“œ ì¤‘/ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.", "id": uid}), 409

    # ì œëª© ê°€ì ¸ì˜¤ê¸°
    data = _load_data()
    queue_item = next((q for q in data["queue"] if q["id"] == uid), None)
    title = queue_item.get("title", "video") if queue_item else "video"

    with _download_lock:
        _download_status[uid] = {
            "status": "queued", "progress": 0, "filename": "",
            "error": "", "title": title, "url": url, "speed": 0,
        }
        _download_queue.append({"uid": uid, "url": url, "title": title})
    _process_download_queue()

    return jsonify({"id": uid, "status": "queued", "title": title})

@app.route("/api/download/status/<uid>")
def download_status(uid):
    s = _download_status.get(uid, {"status": "unknown"})
    return jsonify(s)

@app.route("/api/download/all-status")
def all_download_status():
    """  ëª¨ë“  ë‹¤ìš´ë¡œë“œ ìƒíƒœ ë°˜í™˜"""
    return jsonify(_download_status)

@app.route("/api/download/file/<uid>")
def download_file(uid):
    s = _download_status.get(uid)
    if not s or s["status"] != "done":
        return "íŒŒì¼ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", 404
    filepath = s.get("filename", "")
    if filepath and os.path.exists(filepath):
        return send_file(filepath, as_attachment=True)
    return "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", 404

@app.route("/api/download/clear-done", methods=["POST"])
def clear_done_downloads():
    """ì™„ë£Œ/ì‹¤íŒ¨ ë‹¤ìš´ë¡œë“œ ìƒíƒœ ì •ë¦¬"""
    to_remove = [uid for uid, s in _download_status.items()
                 if s.get("status") in ("done", "error")]
    for uid in to_remove:
        del _download_status[uid]
    return jsonify({"cleared": len(to_remove)})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - ì¿ í‚¤ ìƒíƒœ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/cookies/status")
def cookies_status():
    exists = COOKIES_FILE.exists()
    size = COOKIES_FILE.stat().st_size if exists else 0
    cookie_count = 0
    cookie_errors = []
    if exists:
        try:
            from http.cookiejar import MozillaCookieJar
            cj = MozillaCookieJar(str(COOKIES_FILE))
            cj.load(ignore_discard=True, ignore_expires=True)
            cookie_count = len(cj)
        except Exception as e:
            cookie_errors.append(str(e))

    return jsonify({
        "exists": exists,
        "size": size,
        "count": cookie_count,
        "errors": cookie_errors,
        "path": str(COOKIES_FILE),
        "auto_extract": True,  # yt-dlp ë‚´ì¥ ë¸Œë¼ìš°ì € ì¿ í‚¤ ì¶”ì¶œ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
    })


@app.route("/api/debug", methods=["POST"])
def debug_url():
    """URLì˜ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™€ì„œ ì§„ë‹¨ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    body = request.json
    url = body.get("url", "").strip()
    if not url:
        return jsonify({"error": "URLì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400

    result = {
        "url": url,
        "cookie_file_exists": COOKIES_FILE.exists(),
        "cookie_count": 0,
        "browser_cookie_count": 0,
        "browser_cf_clearance": False,
        "method_used": "",
        "page_length": 0,
        "is_cloudflare_blocked": False,
        "title_found": "",
        "m3u8_found": None,
        "packer_found": False,
        "scripts_count": 0,
        "page_snippet": "",
        "modules": {
            "curl_cffi": False,
            "browser": "",
        },
        "error": None,
    }

    # ëª¨ë“ˆ ê°ì§€
    try:
        __import__('curl_cffi')
        result["modules"]["curl_cffi"] = True
    except ImportError:
        pass
    result["modules"]["browser"] = _get_browser() or "ê°ì§€ ì•ˆë¨"

    # ì¿ í‚¤ ì •ë³´
    if COOKIES_FILE.exists():
        try:
            from http.cookiejar import MozillaCookieJar
            cj = MozillaCookieJar(str(COOKIES_FILE))
            cj.load(ignore_discard=True, ignore_expires=True)
            result["cookie_count"] = len(cj)
        except Exception as e:
            result["error"] = f"ì¿ í‚¤ íŒŒì¼ ì˜¤ë¥˜: {e}"
            return jsonify(result)

    # ë¸Œë¼ìš°ì € ì¿ í‚¤ í™•ì¸ (yt-dlp cookie jar)
    cookie_jar, browser_name = _build_cookie_jar_from_browser()
    if cookie_jar and browser_name:
        parsed_url = urllib.parse.urlparse(url)
        domain = parsed_url.netloc
        parts = domain.split('.')
        base = '.'.join(parts[-2:]) if len(parts) >= 2 else domain
        count = 0
        has_cf = False
        for cookie in cookie_jar:
            cd = cookie.domain.lstrip('.')
            if base in cd or cd in domain:
                count += 1
                if cookie.name == 'cf_clearance':
                    has_cf = True
        result["browser_cookie_count"] = count
        result["browser_cf_clearance"] = has_cf
        result["browser_name"] = browser_name

    # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
    try:
        page, session, method = _fetch_page_with_cf_bypass(url)
        result["method_used"] = method
        result["page_length"] = len(page)

        # Cloudflare ì°¨ë‹¨ ê°ì§€
        result["is_cloudflare_blocked"] = _is_cf_blocked(page)

        # í˜ì´ì§€ ì•ë¶€ë¶„
        result["page_snippet"] = page[:1500]

        # HTML ë¶„ì„
        soup = BeautifulSoup(page, 'html.parser')
        h1 = soup.find('h1')
        if h1:
            result["title_found"] = h1.text.strip()[:100]
        else:
            title_tag = soup.find('title')
            if title_tag:
                result["title_found"] = title_tag.text.strip()[:100]

        # ìŠ¤í¬ë¦½íŠ¸ ë¶„ì„
        scripts = soup.find_all('script')
        result["scripts_count"] = len(scripts)

        for script in scripts:
            if not script.string:
                continue
            content = script.string

            # ì¼ë°˜ M3U8 ê²€ìƒ‰
            m3u8_match = re.search(r"(https?://[^\s\"'<>]+\.m3u8[^\s\"'<>]*)", content)
            if m3u8_match and not result["m3u8_found"]:
                result["m3u8_found"] = m3u8_match.group(1)

            # P.A.C.K.E.R. ë¶„ì„
            packer_match = re.search(
                r"eval\s*\(\s*function\s*\(p,\s*a,\s*c,\s*k,\s*e,\s*d\s*\)\s*\{.*?return\s+p}\s*\((.*)\)\)",
                content, re.DOTALL | re.IGNORECASE
            )
            if packer_match:
                result["packer_found"] = True
                args_str = packer_match.group(1).strip()

                # pcode ì¶”ì¶œ
                pcode_match = re.match(r"(['\"])(.*?)\1\s*,", args_str)
                if pcode_match:
                    quote = pcode_match.group(1)
                    pcode = pcode_match.group(2).replace(f"\\{quote}", quote)
                    result["packer_pcode_len"] = len(pcode)

                    # base, count
                    remaining = args_str[pcode_match.end():]
                    nums = re.findall(r'(\d+)', remaining)
                    base_n = int(nums[0]) if len(nums) >= 1 else 36
                    result["packer_base"] = base_n

                    # kstr ì¶”ì¶œ
                    kstr = ""
                    kstr_match = re.search(
                        r",\s*(['\"])((?:\\.|(?!\1)[^\\\r\n])*)\1\s*\.split\(\s*['\"]" + re.escape('|') + r"['\"]\s*\)",
                        args_str, re.VERBOSE
                    )
                    if kstr_match:
                        kq = kstr_match.group(1)
                        kstr = kstr_match.group(2).replace(f"\\{kq}", kq)
                    keywords = kstr.split('|') if kstr else []
                    result["packer_keywords_count"] = len(keywords)
                    result["packer_keywords_preview"] = keywords[:25]

                    # ì–¸íŒ© ì‹œë„
                    if kstr:
                        unpacked = _unpack_packer(pcode, base_n, 0, kstr)
                        if unpacked:
                            result["packer_unpacked_len"] = len(unpacked)
                            m3u8_up = re.search(r"(https?://[^\s\"'<>]+\.m3u8[^\s\"'<>]*)", unpacked)
                            if m3u8_up:
                                result["m3u8_found"] = m3u8_up.group(1)
                                result["m3u8_method"] = "unpack"

                        # í‚¤ì›Œë“œ ì¬êµ¬ì„± ì‹œë„
                        if not result.get("m3u8_found"):
                            reconstructed = _reconstruct_m3u8_from_keywords(kstr)
                            if reconstructed:
                                result["m3u8_found"] = reconstructed
                                result["m3u8_method"] = "keyword_reconstruct"

        if not result["m3u8_found"]:
            m3u8_page = re.search(r"(https?://[^\s\"'<>]+\.m3u8[^\s\"'<>]*)", page)
            if m3u8_page:
                result["m3u8_found"] = m3u8_page.group(1)

    except Exception as e:
        result["error"] = str(e)

    return jsonify(result)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/settings", methods=["GET"])
def get_settings():
    return jsonify(_load_settings())

@app.route("/api/settings", methods=["PUT"])
def update_settings():
    body = request.json
    settings = _load_settings()
    for key in DEFAULT_SETTINGS:
        if key in body:
            settings[key] = body[key]
    _save_settings(settings)
    # í•­ìƒ ìœ„ ì„¤ì •ì€ ì¦‰ì‹œ ì ìš©
    if "alwaysOnTop" in body and _webview_window:
        try:
            _webview_window.on_top = body["alwaysOnTop"]
        except Exception:
            pass
    return jsonify(settings)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - í•­ìƒ ìœ„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/window/ontop", methods=["POST"])
def toggle_ontop():
    body = request.json
    val = body.get("value", False)
    settings = _load_settings()
    settings["alwaysOnTop"] = val
    _save_settings(settings)
    if _webview_window:
        try:
            _webview_window.on_top = val
            return jsonify({"ok": True, "alwaysOnTop": val})
        except Exception as e:
            return jsonify({"ok": False, "error": str(e)})
    return jsonify({"ok": True, "alwaysOnTop": val, "note": "ë¸Œë¼ìš°ì € ëª¨ë“œì—ì„œëŠ” í•­ìƒìœ„ ë¶ˆê°€"})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - ë°ì´í„° ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/data/export", methods=["GET"])
def export_data():
    data = _load_data()
    return Response(
        json.dumps(data, ensure_ascii=False, indent=2),
        mimetype='application/json',
        headers={'Content-Disposition': 'attachment; filename=streamplayer_backup.json'}
    )

@app.route("/api/data/import", methods=["POST"])
def import_data():
    try:
        if request.content_type and 'multipart' in request.content_type:
            f = request.files.get('file')
            if not f:
                return jsonify({"error": "íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}), 400
            imported = json.loads(f.read().decode('utf-8'))
        else:
            imported = request.json
        if not isinstance(imported, dict):
            return jsonify({"error": "ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤."}), 400
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (queue, playback, heatmaps, settings)
        data = _load_data()
        if "queue" in imported:
            # ê¸°ì¡´ íì— ì—†ëŠ” í•­ëª©ë§Œ ì¶”ê°€
            existing_ids = {q["id"] for q in data.get("queue", [])}
            for item in imported["queue"]:
                if item.get("id") not in existing_ids:
                    data.setdefault("queue", []).append(item)
        if "playback" in imported:
            data.setdefault("playback", {}).update(imported["playback"])
        if "heatmaps" in imported:
            data.setdefault("heatmaps", {}).update(imported["heatmaps"])
        if "settings" in imported:
            data["settings"] = {**DEFAULT_SETTINGS, **imported["settings"]}
        _save_data(data)
        return jsonify({"ok": True, "queue_count": len(data.get("queue", []))})
    except Exception as e:
        return jsonify({"error": f"ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}"}), 400

@app.route("/api/window/size", methods=["POST"])
def save_window_size():
    body = request.json
    settings = _load_settings()
    if body.get("width"):
        settings["windowWidth"] = body["width"]
    if body.get("height"):
        settings["windowHeight"] = body["height"]
    _save_settings(settings)
    return jsonify({"ok": True})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API - ë¸Œë¼ìš°ì € ì¿ í‚¤ ì§ì ‘ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/api/cookies/extract", methods=["POST"])
def extract_cookies_now():
    """ë¸Œë¼ìš°ì €ì—ì„œ ì¿ í‚¤ë¥¼ ì§ì ‘ ì¶”ì¶œí•˜ì—¬ cookies.txtë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    global _detected_browser
    _detected_browser = None  # ìºì‹œ ì´ˆê¸°í™”
    browser = _get_browser()
    if not browser:
        return jsonify({"ok": False, "error": "ê°ì§€ëœ ë¸Œë¼ìš°ì €ê°€ ì—†ìŠµë‹ˆë‹¤."}), 400

    try:
        # yt-dlpì— ì¿ í‚¤ ì¶”ì¶œ + ì €ì¥ì„ ì§ì ‘ ì‹œí‚¤ê¸°
        opts = {
            "quiet": True,
            "no_warnings": True,
            "skip_download": True,
            "cookiesfrombrowser": (browser,),
            "cookiefile": str(COOKIES_FILE),  # ì§ì ‘ ì €ì¥
        }
        ydl = yt_dlp.YoutubeDL(opts)
        cookie_count = len(ydl.cookiejar)
        # yt-dlpëŠ” cookiefile ì˜µì…˜ì´ ìˆìœ¼ë©´ ì¢…ë£Œ ì‹œ ìë™ ì €ì¥
        # ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
        ydl.cookiejar.save(ignore_discard=True, ignore_expires=True)
        print(f"[ì¿ í‚¤ ì¶”ì¶œ] {browser}ì—ì„œ {cookie_count}ê°œ ì¿ í‚¤ ì €ì¥ ì™„ë£Œ")
        return jsonify({
            "ok": True,
            "browser": browser,
            "count": cookie_count,
            "path": str(COOKIES_FILE),
        })
    except Exception as e:
        # ë‹¤ë¥¸ ë¸Œë¼ìš°ì € ì‹œë„
        for alt in ['edge', 'chromium', 'firefox', 'brave']:
            if alt == browser:
                continue
            try:
                opts = {
                    "quiet": True,
                    "no_warnings": True,
                    "skip_download": True,
                    "cookiesfrombrowser": (alt,),
                    "cookiefile": str(COOKIES_FILE),
                }
                ydl = yt_dlp.YoutubeDL(opts)
                cookie_count = len(ydl.cookiejar)
                ydl.cookiejar.save(ignore_discard=True, ignore_expires=True)
                print(f"[ì¿ í‚¤ ì¶”ì¶œ] {alt}ì—ì„œ {cookie_count}ê°œ ì¿ í‚¤ ì €ì¥ ì™„ë£Œ (í´ë°±)")
                return jsonify({
                    "ok": True,
                    "browser": alt,
                    "count": cookie_count,
                    "path": str(COOKIES_FILE),
                })
            except Exception:
                continue
        return jsonify({"ok": False, "error": f"ì¿ í‚¤ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"}), 500

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìë™ ì €ì¥/ë°±ì—… (ì£¼ê¸°ì  + ì¢…ë£Œ ì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import atexit

def _periodic_backup():
    """5ë¶„ë§ˆë‹¤ data.jsonì„ ìë™ ë°±ì—…í•©ë‹ˆë‹¤."""
    import time as _time
    while True:
        _time.sleep(300)  # 5ë¶„
        try:
            data = _load_data()
            _save_data(data)
            print("  [ìë™ë°±ì—…] ì£¼ê¸°ì  ë°±ì—… ì™„ë£Œ")
        except Exception as e:
            print(f"  [ìë™ë°±ì—…] ì‹¤íŒ¨: {e}")

def _shutdown_save():
    """í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ìµœì¢… ì €ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        data = _load_data()
        _save_data(data)
        print("  [ì¢…ë£Œì €ì¥] ìµœì¢… ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"  [ì¢…ë£Œì €ì¥] ì‹¤íŒ¨: {e}")

atexit.register(_shutdown_save)
# ë°±ì—… ìŠ¤ë ˆë“œ ì‹œì‘
threading.Thread(target=_periodic_backup, daemon=True, name="AutoBackup").start()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    # ëª¨ë“ˆ ìƒíƒœ í™•ì¸
    mods = []
    for m in ["curl_cffi"]:
        try:
            __import__(m)
            mods.append(f"  âœ“ {m}")
        except ImportError:
            mods.append(f"  âœ— {m} (ë¯¸ì„¤ì¹˜)")
    try:
        from yt_dlp.cookies import extract_cookies_from_browser
        mods.append("  âœ“ yt-dlp ë¸Œë¼ìš°ì € ì¿ í‚¤ ì¶”ì¶œ")
    except ImportError:
        mods.append("  âœ— yt-dlp ë¸Œë¼ìš°ì € ì¿ í‚¤ ì¶”ì¶œ (yt-dlp ì—…ë°ì´íŠ¸ í•„ìš”)")
    print("=" * 50)
    print("  StreamPlayer ì„œë²„ ì‹œì‘")
    print(f"  http://localhost:5000")
    print(f"  ì¿ í‚¤: {'âœ“ cookies.txt ê°ì§€ë¨' if COOKIES_FILE.exists() else 'ë¸Œë¼ìš°ì €ì—ì„œ ìë™ ì¶”ì¶œ'}")
    print(f"  ë‹¤ìš´ë¡œë“œ í´ë”: {DOWNLOADS_DIR}")
    print("  ëª¨ë“ˆ:")
    for m in mods:
        print(m)
    print("=" * 50)
    # ë°±ê·¸ë¼ìš´ë“œ ì‚¬ì „ ì¶”ì¶œ ìŠ¤ë ˆë“œ ì‹œì‘
    threading.Thread(target=_background_preextract, daemon=True).start()
    app.run(host="127.0.0.1", port=5000, debug=True, threaded=True)
